{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Welcome to our project report! ‚ú®üß™**\n",
    "\n",
    "## üåé *Introduction*\n",
    "\n",
    "The pKaPredict package is a cheminformatics tool designed to estimate the acid dissociation constant (pKa) of chemical compounds based on their molecular structure. The [pKa](https://chemistrytalk.org/what-is-pka/) value is a fundamental physicochemical property that indicates the strength of an acid in an aqueous solution. Specifically, it represents the pH at which a molecule exists in equilibrium between its protonated and deprotonated forms. It is defined as the negative logarithm (base 10) of a compound‚Äôs acid dissociation constant in water (Ka). Accurate knowledge of pKa is critical in [fields](https://www.pion-inc.com/blog/what-is-pka-and-how-is-it-used-in-drug-development) such as drug development, environmental chemistry, and molecular biology, where ionization affects solubility, permeability, binding affinity, and reactivity. <br>\n",
    "\n",
    "*pKaPredict* leverages computational chemistry and machine learning to deliver fast and reproducible predictions of pKa values. The pipeline begins by converting SMILES (Simplified Molecular Input Line Entry System) strings into molecular descriptors using the RDKit cheminformatics library. These descriptors encode key structural and electronic features of molecules and serve as input variables for the predictive model. <br>\n",
    "\n",
    "The descriptor generation process is handled by the **smiles_to_rdkit_descriptors** function, which transforms SMILES strings into a comprehensive set of numerical descriptors. These include 2D physicochemical properties, topological indices, and fragment-based counts. <br>\n",
    "\n",
    "In the next stage, the pre-trained regression model is loaded using the load_model function, and pKa predictions are generated through the **predict_pKa** function. The model was trained on a dataset containing molecules with experimentally determined pKa values ranging approximately from 2 to 12. These molecules were first converted into molecular descriptors via the **RDKit_descriptors** function. Among the models tested from the LazyPredict library, the LGBMRegressor yielded the best performance and was selected as the final model. <br>\n",
    "\n",
    "Prior to model training, the dataset was cleaned by removing duplicates and missing values using the **clean_and_visualize_pka** function. Moreover, supplementary tools such as the **plot_data** enable the visualization of predicted versus experimental values and generate graphical summaries to support model evaluation and interpretation. <br>\n",
    "\n",
    "The central component of the package is the **predict_pKa** function, which integrates the aforementioned steps into a streamlined workflow, enabling users to perform end-to-end pKa predictions from molecular input to final output.\n",
    "\n",
    "\n",
    "## üöÄ *Overview*\n",
    "ü§Ø Acquiring Dataset <br>\n",
    "üßπ Cleaning Dataset <br>\n",
    "üõü Saving the cleaned data to a csv file <br>\n",
    "ü§ì Computation of RDKit Molecular Descriptors <br>\n",
    "üí° Formatting the dataset for machine learning <br>\n",
    "üïπÔ∏è Machine learning model selection <br>\n",
    "üå≤ Machine learning model ü•á: ExtraTreesRegressor <br>\n",
    "ü§ñ Machine learning model ü•à : LGBMRegressor <br>\n",
    "üßê Comparison of the two machine learning models <br>\n",
    "üßÖ Saving the LGBMRegressor trained model <br>\n",
    "ü©∑ Results: usage of this trained machine learning model <br>\n",
    "üí¨ Discussion\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Import statements**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standard library\n",
    "import os\n",
    "import time\n",
    "import pickle\n",
    "from pathlib import Path\n",
    "\n",
    "# Data handling\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Plotting and visualization\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import seaborn as sn  # Included both in case both are used differently\n",
    "import matplotlib.patches as mpatches\n",
    "\n",
    "# Web and display\n",
    "import requests\n",
    "import IPython.display as ipd\n",
    "\n",
    "# Chemistry\n",
    "from rdkit import Chem\n",
    "from rdkit.Chem import AllChem, Descriptors\n",
    "from rdkit.ML.Descriptors import MoleculeDescriptors\n",
    "from rdkit.ML.Descriptors.MoleculeDescriptors import MolecularDescriptorCalculator\n",
    "\n",
    "# Machine learning and preprocessing\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.ensemble import ExtraTreesRegressor\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "from sklearn.feature_selection import SelectKBest, f_regression\n",
    "\n",
    "# LightGBM\n",
    "from lightgbm import LGBMRegressor\n",
    "\n",
    "# LazyPredict\n",
    "from lazypredict.Supervised import LazyRegressor\n",
    "\n",
    "# Statistics\n",
    "from scipy import stats\n",
    "\n",
    "# Progress bar\n",
    "from tqdm import tqdm\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üßπ *Acquiring and Cleaning the Dataset*\n",
    "In a first step, the training [pKa dataset](https://github.com/cbio3lab/pKa/blob/main/Data/test_acids_bases_descfinal_nozwitterions.csv) is acquired from cbio3lab's repository, initially extracted from the Harvard [dataverse](https://dataverse.harvard.edu/dataset.xhtml?persistentId=doi:10.7910/DVN/6A67L9). \n",
    "\n",
    "\n",
    "‚úÖ Downloads the data directly into the working directory <br>\n",
    "‚úÖ Opens the file and verifies its existence as well as displays a preview of the latter <br>\n",
    "‚úÖ Prints initial dataset shape <br> \n",
    "‚úÖ Counts and removes missing values (NaN) and duplicates <br>\n",
    "‚úÖ Prints final dataset shape after cleaning  <br>\n",
    "‚úÖ Generates a histogram to visualize pKa value distribution  <br>\n",
    "‚úÖ Saves the cleaned dataset as a CSV file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "ename": "UnboundLocalError",
     "evalue": "cannot access local variable 'Path' where it is not associated with a value",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mUnboundLocalError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[45], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mpkapredict\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdata_preprocessing\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m download_raw_dataset, preview_data, clean_and_visualize_pka\n\u001b[0;32m----> 3\u001b[0m file_path \u001b[38;5;241m=\u001b[39m \u001b[43mdownload_raw_dataset\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      4\u001b[0m df \u001b[38;5;241m=\u001b[39m preview_data()\n\u001b[1;32m      5\u001b[0m cleaned_df \u001b[38;5;241m=\u001b[39m clean_and_visualize_pka(df)\n",
      "File \u001b[0;32m~/Desktop/pKaPredict/pKaPredict/src/pkapredict/data_preprocessing.py:27\u001b[0m, in \u001b[0;36mdownload_raw_dataset\u001b[0;34m(url, filename, levels_up, data_folder)\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     15\u001b[0m \u001b[38;5;124;03mDownloads a CSV file from a given URL and saves it in the specified data directory.\u001b[39;00m\n\u001b[1;32m     16\u001b[0m \u001b[38;5;124;03m\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[38;5;124;03m    Path: Path to the downloaded file.\u001b[39;00m\n\u001b[1;32m     25\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     26\u001b[0m \u001b[38;5;66;03m# Get repository root by going up `levels_up` directories\u001b[39;00m\n\u001b[0;32m---> 27\u001b[0m repo_root \u001b[38;5;241m=\u001b[39m \u001b[43mPath\u001b[49m\u001b[38;5;241m.\u001b[39mcwd()\n\u001b[1;32m     28\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(levels_up):\n\u001b[1;32m     29\u001b[0m     repo_root \u001b[38;5;241m=\u001b[39m repo_root\u001b[38;5;241m.\u001b[39mparent\n",
      "\u001b[0;31mUnboundLocalError\u001b[0m: cannot access local variable 'Path' where it is not associated with a value"
     ]
    }
   ],
   "source": [
    "from pkapredict.data_preprocessing import download_raw_dataset, preview_data, clean_and_visualize_pka\n",
    "\n",
    "file_path = download_raw_dataset()\n",
    "df = preview_data()\n",
    "cleaned_df = clean_and_visualize_pka(df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "üìä Presentation of the pKa dataset as a histogram:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Generate histogram for pKa distribution\n",
    "print(\"\\nüìä Generating histogram for pKa distribution...\\n\")\n",
    "\n",
    "# Set white background \n",
    "sns.set_theme(style=\"white\")\n",
    "\n",
    "# Create figure\n",
    "plt.figure(figsize=(8, 5))\n",
    "sns.histplot(data=cleaned_df, x=\"pka\", binwidth=0.5, kde=True, color=\"pink\")  \n",
    "\n",
    "\n",
    "plt.xlabel(\"pKa\", fontsize=12)\n",
    "plt.ylabel(\"Frequency\", fontsize=12)\n",
    "plt.title(\"Distribution of pKa Values\", fontsize=14)\n",
    "plt.grid(False)  \n",
    "\n",
    "# Define save path in the specified folder\n",
    "save_path = os.path.join(\"..\", \"notebooks\", \"Plots\", \"pKa_distribution_of_the_dataset.png\")\n",
    "\n",
    "# Save histogram as PNG\n",
    "plt.savefig(save_path, dpi=300, bbox_inches=\"tight\")\n",
    "\n",
    "# Show plot\n",
    "plt.show()\n",
    "\n",
    "# Success message\n",
    "print(f\"üìÅ Histogram saved successfully at: {save_path}\")\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ü§ì *Computation of RDKit Molecular Descriptors*\n",
    "Molecular descriptors are essential for pKa prediction using machine learning because they provide numerical representations of molecular structures, enabling the model to identify patterns and correlations between molecular features and pKa values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from pkapredict.RDkit_descriptors import RDkit_descriptors\n",
    "\n",
    "# Load cleaned data containing SMILES\n",
    "file_path_cleaned = os.path.join(\"..\", \"data\", \"pKadataset_cleaned.csv\")\n",
    "cleaned_data = pd.read_csv(file_path_cleaned)\n",
    "\n",
    "# Compute descriptors\n",
    "Mol_descriptors, desc_names = RDkit_descriptors(cleaned_data['Smiles'])\n",
    "\n",
    "# Create DataFrame with descriptors and add SMILES column\n",
    "df_descriptors = pd.DataFrame(Mol_descriptors, columns=desc_names)\n",
    "df_descriptors.insert(0, \"Smiles\", cleaned_data['Smiles'])  # Insert Smiles as the first column\n",
    "\n",
    "# Save descriptors as CSV file in pkapredict/data\n",
    "save_path = os.path.join(\"..\", \"data\", \"Data_pKa_Descriptors.csv\")\n",
    "df_descriptors.to_csv(save_path, index=False)\n",
    "\n",
    "print(\"Descriptor computation completed.\")\n",
    "print(f\"Training data descriptors saved as '{save_path}'.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üí° *Formatting the dataset for machine learning*\n",
    "This section of the code prepares the dataset for training a machine learning model by splitting it into training and validation sets and standardizing the features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Define file paths\n",
    "file_path1 = os.path.join(\"..\", \"data\", \"Data_pKa_Descriptors.csv\")  # Descriptors (X)\n",
    "file_path2 = os.path.join(\"..\", \"data\", \"pKadataset_cleaned.csv\")  # pKa values (y)\n",
    "\n",
    "# Load the datasets\n",
    "df_descriptors = pd.read_csv(file_path1)  # Features (X)\n",
    "df_pKa = pd.read_csv(file_path2)  # Target values (y)\n",
    "\n",
    "# Display first few rows of both datasets\n",
    "print(\"Descriptors DataFrame:\\n\", df_descriptors.head())\n",
    "print(\"pKa DataFrame:\\n\", df_pKa.head())\n",
    "\n",
    "# Ensure the 'Smiles' column is present in both\n",
    "common_column = 'Smiles'\n",
    "\n",
    "# Merge descriptors with pKa values using 'Smiles'\n",
    "df_merged = df_descriptors.merge(df_pKa[['pka', common_column]], on=common_column)\n",
    "\n",
    "# Drop non-numeric columns (Smiles)\n",
    "X = df_merged.drop(columns=['pka', 'Smiles'])  # Drop pKa (target) and Smiles (string)\n",
    "y = df_merged['pka']  # Target variable (pKa values)\n",
    "\n",
    "# Verify all columns are numeric\n",
    "print(\"X columns before standardization:\", X.dtypes)\n",
    "\n",
    "# Split data into training (90%) and validation (10%) sets\n",
    "X_train, X_valid, y_train, y_valid = train_test_split(X, y, test_size=0.1, random_state=42)\n",
    "\n",
    "# Standardize the features\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_valid_scaled = scaler.transform(X_valid)\n",
    "\n",
    "# Print shape to confirm data processing\n",
    "print(f\"Training set shape: {X_train.shape}, Validation set shape: {X_valid.shape}\")\n",
    "\n",
    "# Success message üéÄ\n",
    "print(\"The training and validation sets have been successfully created üéÄ\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üïπÔ∏è *Machine learning model selection*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# =============================\n",
    "# Run LazyRegressor \n",
    "# =============================\n",
    "\n",
    "# Initialize LazyRegressor\n",
    "lregs = LazyRegressor(verbose=0, ignore_warnings=True, custom_metric=None, random_state=42)\n",
    "\n",
    "# Fit models on the pKa dataset\n",
    "models, prediction_tests = lregs.fit(X_train_scaled, X_valid_scaled, y_train, y_valid)\n",
    "\n",
    "# =============================\n",
    "# Visualize Top Models\n",
    "# =============================\n",
    "\n",
    "# Sort models by R-squared score (best performing first)\n",
    "models_sorted = models.sort_values(by=\"R-Squared\", ascending=False)\n",
    "\n",
    "# Plot top 10 models\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.barh(models_sorted.index[:10], models_sorted[\"R-Squared\"][:10], align='center', color='pink')\n",
    "plt.xlabel(\"R-Squared Score\")\n",
    "plt.ylabel(\"Machine Learning Models\")\n",
    "plt.title(\"Top 10 Machine Learning Models for pKa Prediction\")\n",
    "plt.gca().invert_yaxis()  # Best model on top\n",
    "\n",
    "\n",
    "# Define save path in the specified folder\n",
    "save_path = os.path.join(\"..\", \"notebooks\", \"Plots\", \"Top10MLModels.png\")\n",
    "\n",
    "# Save figure as PNG in the correct directory\n",
    "plt.savefig(save_path, dpi=300, bbox_inches='tight')\n",
    "\n",
    "# Success message\n",
    "print(f\"üìÅ Figure saved successfully at: {save_path}\")\n",
    "\n",
    "\n",
    "\n",
    "# Show plot\n",
    "plt.show()\n",
    "\n",
    "print(\"Model selection completed! üéØ The best-performing models have been visualized and saved as 'Top10MLModels.png'.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üå≤ *Machine learning model ü•á : ExtraTreesRegressor*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# =============================\n",
    "# 1st Grid Search for ExtraTreesRegressor\n",
    "# =============================\n",
    "\n",
    "# Define initial hyperparameter grid\n",
    "params = {\n",
    "    \"max_depth\": list(range(15, 26, 5)),  # Trying depths between 10 and 30\n",
    "    \"n_estimators\": list(range(100, 500, 100)),  # Number of trees between 100 and 500\n",
    "    \"min_samples_split\": [2, 5, 10],  # Minimum samples to split a node\n",
    "}\n",
    "\n",
    "# Initialize GridSearchCV\n",
    "grid_search = GridSearchCV(\n",
    "    ExtraTreesRegressor(random_state=42),\n",
    "    param_grid=params,\n",
    "    cv=3,\n",
    "    verbose=1,\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "# Fit GridSearchCV to the training data\n",
    "grid_search.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Print best hyperparameters\n",
    "print(\"Optimized parameters for ExtraTreesRegressor:\", grid_search.best_params_)\n",
    "\n",
    "# =============================\n",
    "# 2nd Grid Search for Further Optimization\n",
    "# =============================\n",
    "\n",
    "# Extract best hyperparameters from first search\n",
    "best_params = grid_search.best_params_\n",
    "\n",
    "# Define refined hyperparameter grid\n",
    "params_bst = {\n",
    "    \"max_depth\": list(range(best_params[\"max_depth\"] - 2, best_params[\"max_depth\"] + 3, 1)),\n",
    "    \"n_estimators\": list(range(best_params[\"n_estimators\"] - 50, best_params[\"n_estimators\"] + 100, 50)),\n",
    "    \"min_samples_split\": [best_params[\"min_samples_split\"] - 1, best_params[\"min_samples_split\"], best_params[\"min_samples_split\"] + 1],\n",
    "}\n",
    "\n",
    "# Initialize refined GridSearchCV\n",
    "grid_search_bst = GridSearchCV(\n",
    "    ExtraTreesRegressor(random_state=42),\n",
    "    param_grid=params_bst,\n",
    "    cv=3,\n",
    "    verbose=1,\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "# Fit GridSearchCV to the training data\n",
    "grid_search_bst.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Print the best hyperparameters\n",
    "print(\"The best parameters after further optimization:\", grid_search_bst.best_params_)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The hyperparameter tuning for ExtraTreesRegressor was conducted in two stages to identify the optimal parameters for predicting pKa values efficiently. In the first grid search, 36 different hyperparameter combinations were tested using 3-fold cross-validation (cv=3) instead of 5-fold to significantly reduce computational time while maintaining accuracy. This resulted in an initial best model with max_depth = 25, min_samples_split = 2, and n_estimators = 100. To further refine the model, a second grid search was conducted, focusing on a narrower range of max_depth (15 to 26 instead of 10 to 31) and adjusting n_estimators, leading to the final optimized values: max_depth = 23, min_samples_split = 2, and n_estimators = 150. These refinements balanced model complexity and efficiency, ensuring improved generalization while significantly reducing training time. üöÄ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# =============================\n",
    "# Train ExtraTreesRegressor with Optimized Parameters\n",
    "# =============================\n",
    "\n",
    "# Define the best parameters found from GridSearchCV\n",
    "best_params = {'max_depth': 23, 'min_samples_split': 2, 'n_estimators': 150}\n",
    "\n",
    "# Initialize the model with optimized parameters\n",
    "model = ExtraTreesRegressor(\n",
    "    n_estimators=best_params[\"n_estimators\"],\n",
    "    max_depth=best_params[\"max_depth\"],\n",
    "    min_samples_split=best_params[\"min_samples_split\"],\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "# Train the model\n",
    "model.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Make predictions on the validation set\n",
    "y_preds = model.predict(X_valid_scaled)\n",
    "\n",
    "# =============================\n",
    "# Define Function for Plotting\n",
    "# =============================\n",
    "\n",
    "from pkapredict.plot_data import plot_data\n",
    "\n",
    "# =============================\n",
    "# Plot and Save Results\n",
    "# =============================\n",
    "\n",
    "# Plot validation results\n",
    "plot_data(y_valid, y_preds, \"Validation Data - ExtraTreesRegressor\")\n",
    "\n",
    "# Define save path in the specified folder\n",
    "save_path = os.path.join(\"..\", \"notebooks\", \"Plots\", \"validation_ExtraTrees.png\")\n",
    "\n",
    "# Save figure as PNG in the correct directory\n",
    "plt.savefig(save_path, dpi=300, bbox_inches='tight')\n",
    "\n",
    "# Success message\n",
    "print(\"Plot saved as validation_ExtraTrees.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "üì∂ Optimization of a supplementary parameter of this model through the selection of the most relavent descriptors to pKa prediction using SelectKBest. The following code plots all R¬≤ values as a function of k values and prints the optimal one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Define range of k values (from 1 to total number of features, step 1)\n",
    "k_values = list(range(1, X_train_scaled.shape[1] + 1))\n",
    "\n",
    "# Store R¬≤ scores for each k\n",
    "r2_scores = []\n",
    "\n",
    "# Loop through k values and train ExtraTreesRegressor\n",
    "for k in k_values:\n",
    "    # Select top k features\n",
    "    selector = SelectKBest(score_func=f_regression, k=k)\n",
    "    X_train_selected = selector.fit_transform(X_train_scaled, y_train)\n",
    "    X_valid_selected = selector.transform(X_valid_scaled)\n",
    "\n",
    "    # Train ExtraTreesRegressor with selected features\n",
    "    model = ExtraTreesRegressor(n_estimators=150, max_depth=23, min_samples_split=2, random_state=42)\n",
    "    model.fit(X_train_selected, y_train)\n",
    "\n",
    "    # Make predictions\n",
    "    y_preds = model.predict(X_valid_selected)\n",
    "\n",
    "    # Compute R¬≤ score\n",
    "    r2 = r2_score(y_valid, y_preds)\n",
    "    r2_scores.append(r2)\n",
    "\n",
    "# Find the optimal k value\n",
    "optimal_k = k_values[np.argmax(r2_scores)]\n",
    "best_r2 = max(r2_scores)\n",
    "\n",
    "# Print optimal k value with highest R¬≤\n",
    "print(f\"Optimal k: {optimal_k} with Highest R¬≤: {best_r2:.4f}\")\n",
    "\n",
    "# Plot R¬≤ Scores vs. k Values\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(k_values, r2_scores, marker='o', linestyle='-', color='lightpink')\n",
    "plt.xlabel(\"Number of Features (k)\", color=\"black\")\n",
    "plt.ylabel(\"R¬≤ Score\", color=\"black\")\n",
    "plt.title(\"Optimal k for SelectKBest Feature Selection\", color=\"black\")\n",
    "plt.gca().set_facecolor('white')  \n",
    "plt.tick_params(axis='both', colors='black')  \n",
    "\n",
    "# Define save path in the specified folder\n",
    "save_path = os.path.join(\"..\", \"notebooks\", \"Plots\", \"optimalkvalue.png\")\n",
    "\n",
    "# Save figure as PNG in the correct directory\n",
    "plt.savefig(save_path, dpi=300, bbox_inches='tight')\n",
    "\n",
    "\n",
    "print(\"Plot saved as optimalkvalue.png\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "üí™üèª Plot of the data with the optimized machine leaning parameters of this model!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# =============================\n",
    "# Feature Selection with SelectKBest (k=95)\n",
    "# =============================\n",
    "\n",
    "k = 95  # Number of best features to select\n",
    "selector = SelectKBest(score_func=f_regression, k=k)\n",
    "X_train_selected = selector.fit_transform(X_train_scaled, y_train)\n",
    "X_valid_selected = selector.transform(X_valid_scaled)\n",
    "\n",
    "# =============================\n",
    "# Train ExtraTreesRegressor with Optimized Parameters\n",
    "# =============================\n",
    "\n",
    "# Define the best parameters found from GridSearchCV\n",
    "best_params = {'max_depth': 23, 'min_samples_split': 2, 'n_estimators': 150}\n",
    "\n",
    "# Initialize the model with optimized parameters\n",
    "model = ExtraTreesRegressor(\n",
    "    n_estimators=best_params[\"n_estimators\"],\n",
    "    max_depth=best_params[\"max_depth\"],\n",
    "    min_samples_split=best_params[\"min_samples_split\"],\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "# Train the model on the selected features\n",
    "model.fit(X_train_selected, y_train)\n",
    "\n",
    "# Make predictions on the validation set\n",
    "y_preds = model.predict(X_valid_selected)\n",
    "\n",
    "# =============================\n",
    "# Define Function for Plotting\n",
    "# =============================\n",
    "\n",
    "from pkapredict.plot_data import plot_data\n",
    "\n",
    "# =============================\n",
    "# Plot and Save Results\n",
    "# =============================\n",
    "\n",
    "# Plot validation results\n",
    "plot_data(y_valid, y_preds, \"Validation Optimized Data - ExtraTreesRegressor\")\n",
    "\n",
    "# Define save path in the specified folder\n",
    "save_path = os.path.join(\"..\", \"notebooks\", \"Plots\", \"validationoptimized_ExtraTrees.png\")\n",
    "\n",
    "# Save figure as PNG in the correct directory\n",
    "plt.savefig(save_path, dpi=300, bbox_inches='tight')\n",
    "\n",
    "# Success message\n",
    "print(\"Plot saved as validationoptimized_ExtraTrees.png\")\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "‚ö°Ô∏è Computation of statistics for this model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# =============================\n",
    "# Compute Model Performance Metrics\n",
    "# =============================\n",
    "\n",
    "# Compute mean error\n",
    "mean_error = np.mean(y_preds - y_valid)\n",
    "\n",
    "# Compute variance of the error\n",
    "variance_error = np.var(y_preds - y_valid)\n",
    "\n",
    "# Compute mean predicted value\n",
    "mean_predicted = np.mean(y_preds)\n",
    "\n",
    "# Compute Mean Squared Error (MSE)\n",
    "mse = mean_squared_error(y_valid, y_preds)\n",
    "\n",
    "# Compute R¬≤ score\n",
    "r2 = r2_score(y_valid, y_preds)\n",
    "\n",
    "# Compute 95% confidence interval for predictions\n",
    "confidence = 0.95\n",
    "n = len(y_preds)\n",
    "std_err = stats.sem(y_preds - y_valid)\n",
    "margin_of_error = std_err * stats.t.ppf((1 + confidence) / 2., n-1)\n",
    "confidence_interval = (mean_error - margin_of_error, mean_error + margin_of_error)\n",
    "\n",
    "# Print computed metrics\n",
    "print(f\"Mean Error: {mean_error:.4f}\")\n",
    "print(f\"Variance of Error: {variance_error:.4f}\")\n",
    "print(f\"Mean Predicted Value: {mean_predicted:.4f}\")\n",
    "print(f\"MSE: {mse:.4f}\")\n",
    "print(f\"R¬≤: {r2:.4f}\")\n",
    "print(f\"95% Confidence Interval: {confidence_interval}\")\n",
    "\n",
    "# Create a DataFrame for storing the results\n",
    "stats_df = pd.DataFrame({\n",
    "    \"Metric\": [\"Mean Error\", \"Variance\", \"Mean Predicted Value\", \"MSE\", \"R¬≤\", \"95% CI Lower\", \"95% CI Upper\"],\n",
    "    \"Value\": [mean_error, variance_error, mean_predicted, mse, r2, confidence_interval[0], confidence_interval[1]]\n",
    "})\n",
    "\n",
    "\n",
    "stats_save_path = os.path.join(\"..\", \"data\", \"ExtraTreesRegressor_stats.csv\")\n",
    "stats_df.to_csv(stats_save_path, index=False)\n",
    "print(f\"Table saved at: {stats_save_path}\")\n",
    "\n",
    "# Display as an interactive HTML table\n",
    "ipd.display(ipd.HTML(stats_df.to_html(classes='table table-striped', escape=False)))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ü§ñ *Machine learning model ü•à : LGBMRegressor*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# =============================\n",
    "# Load and Prepare Data\n",
    "# =============================\n",
    "\n",
    "# Define file paths\n",
    "file_path1 = os.path.join(\"..\", \"data\", \"Data_pKa_Descriptors.csv\")  # Descriptors (X)\n",
    "file_path2 = os.path.join(\"..\", \"data\", \"pKadata_cleaned.csv\")  # pKa values (y)\n",
    "\n",
    "# Load the datasets\n",
    "df_descriptors = pd.read_csv(file_path1)  # Features (X)\n",
    "df_pKa = pd.read_csv(file_path2)  # Target values (y)\n",
    "\n",
    "# Merge descriptors with pKa values using 'Smiles'\n",
    "df_merged = df_descriptors.merge(df_pKa[['pka', 'Smiles']], on='Smiles')\n",
    "\n",
    "# Drop non-numeric columns (Smiles)\n",
    "X = df_merged.drop(columns=['pka', 'Smiles'])  # Drop pKa (target) and Smiles (string)\n",
    "y = df_merged['pka']  # Target variable (pKa values)\n",
    "\n",
    "# Split data into training (90%) and validation (10%) sets\n",
    "X_train, X_valid, y_train, y_valid = train_test_split(X, y, test_size=0.1, random_state=42)\n",
    "\n",
    "# Standardize the features\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_valid_scaled = scaler.transform(X_valid)\n",
    "\n",
    "# Convert back to DataFrame to avoid warnings in prediction\n",
    "X_train_scaled_df = pd.DataFrame(X_train_scaled, columns=X.columns)\n",
    "X_valid_scaled_df = pd.DataFrame(X_valid_scaled, columns=X.columns)\n",
    "\n",
    "# =============================\n",
    "# 1st Grid Search for LGBMRegressor (Optimized for Speed)\n",
    "# =============================\n",
    "\n",
    "# Start timing\n",
    "start_time = time.time()\n",
    "\n",
    "# Define initial hyperparameter grid with reduced values for speed\n",
    "params = {\n",
    "    \"max_depth\": [10, 15],  # Smaller range\n",
    "    \"n_estimators\": [100, 200],  # Fewer estimators\n",
    "    \"learning_rate\": [0.05, 0.1],  # Limited choices\n",
    "    \"num_leaves\": [20, 31],  # Small variation\n",
    "    \"min_child_samples\": [10, 20]  # Small variation\n",
    "}\n",
    "\n",
    "# Initialize GridSearchCV\n",
    "grid_search = GridSearchCV(\n",
    "    LGBMRegressor(random_state=42, verbose=-1),  \n",
    "    param_grid=params,\n",
    "    cv=2,  \n",
    "    verbose=0,  \n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "# Fit GridSearchCV to the training data\n",
    "grid_search.fit(X_train_scaled_df, y_train)\n",
    "\n",
    "# Print best hyperparameters\n",
    "print(\"Optimized parameters for LGBMRegressor:\", grid_search.best_params_)\n",
    "\n",
    "# =============================\n",
    "# 2nd Grid Search for Further Optimization\n",
    "# =============================\n",
    "\n",
    "# Extract best hyperparameters from first search\n",
    "best_params = grid_search.best_params_\n",
    "\n",
    "# Define refined hyperparameter grid\n",
    "params_bst = {\n",
    "    \"max_depth\": [best_params[\"max_depth\"] - 1, best_params[\"max_depth\"], best_params[\"max_depth\"] + 1],\n",
    "    \"n_estimators\": [best_params[\"n_estimators\"] - 50, best_params[\"n_estimators\"] + 50],\n",
    "    \"learning_rate\": [best_params[\"learning_rate\"] * 0.9, best_params[\"learning_rate\"], best_params[\"learning_rate\"] * 1.1],\n",
    "    \"num_leaves\": [best_params[\"num_leaves\"] - 5, best_params[\"num_leaves\"] + 5],\n",
    "    \"min_child_samples\": [best_params[\"min_child_samples\"] - 5, best_params[\"min_child_samples\"] + 5]\n",
    "}\n",
    "\n",
    "# Initialize refined GridSearchCV\n",
    "grid_search_bst = GridSearchCV(\n",
    "    LGBMRegressor(random_state=42, verbose=-1),  \n",
    "    param_grid=params_bst,\n",
    "    cv=2,  \n",
    "    verbose=0, \n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "# Fit GridSearchCV to the training data\n",
    "grid_search_bst.fit(X_train_scaled_df, y_train)\n",
    "\n",
    "# Print the best hyperparameters\n",
    "print(\"The best parameters after further optimization:\", grid_search_bst.best_params_)\n",
    "\n",
    "# End timing\n",
    "end_time = time.time()\n",
    "print(f\"Total execution time: {end_time - start_time:.2f} seconds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ü•≥ Implementation of the optimized parameters and plotting of the data distribution as well as the associated R¬≤ value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# =============================\n",
    "# Train LGBMRegressor with Optimized Parameters\n",
    "# =============================\n",
    "\n",
    "# Define the best parameters found from GridSearchCV\n",
    "best_params = {\n",
    "    'learning_rate': 0.045000000000000005,\n",
    "    'max_depth': 9,\n",
    "    'min_child_samples': 5,\n",
    "    'n_estimators': 250,\n",
    "    'num_leaves': 15\n",
    "}\n",
    "\n",
    "# Initialize the model with optimized parameters\n",
    "model2 = LGBMRegressor(\n",
    "    learning_rate=best_params['learning_rate'],\n",
    "    max_depth=best_params['max_depth'],\n",
    "    min_child_samples=best_params['min_child_samples'],\n",
    "    n_estimators=best_params['n_estimators'],\n",
    "    num_leaves=best_params['num_leaves'],\n",
    "    random_state=42,\n",
    "    verbose=-1  \n",
    ")\n",
    "\n",
    "# Train the model\n",
    "model2.fit(X_train_scaled_df, y_train)\n",
    "\n",
    "# Make predictions on the validation set\n",
    "y_preds = model2.predict(X_valid_scaled_df)\n",
    "\n",
    "# =============================\n",
    "# Define Function for Plotting\n",
    "# =============================\n",
    "\n",
    "\n",
    "\n",
    "# Save figure\n",
    "plt.savefig(save_path, dpi=300, bbox_inches='tight')\n",
    "    \n",
    "\n",
    "\n",
    "# =============================\n",
    "# Plot and Save Results\n",
    "# =============================\n",
    "\n",
    "# Define save path in the specified folder\n",
    "save_path = os.path.join(\"..\", \"notebooks\", \"Plots\", \"validation_LGBMRegressor.png\")\n",
    "\n",
    "# Plot validation results and display the plot\n",
    "plot_data(y_valid, y_preds, \"Validation Data - LGBMRegressor\")\n",
    "\n",
    "# Success message\n",
    "print(\"Plot saved as validation_LGBMRegressor.png\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "üì∂ Optimization of a supplementary parameter of this model by selecting the most relavent descriptors to pKa prediction using SelectKBest. The following code plots all R¬≤ values as a function of k values and prints the optimal one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Define range of k values (from 1 to total number of features, step 1)\n",
    "k_values = list(range(1, X_train_scaled_df.shape[1] + 1))\n",
    "\n",
    "# Store R¬≤ scores for each k\n",
    "r2_scores = []\n",
    "\n",
    "# Loop through k values and train LGBMRegressor\n",
    "for k in k_values:\n",
    "    # Select top k features\n",
    "    selector = SelectKBest(score_func=f_regression, k=k)\n",
    "    X_train_selected = selector.fit_transform(X_train_scaled_df, y_train)\n",
    "    X_valid_selected = selector.transform(X_valid_scaled_df)\n",
    "\n",
    "    # Convert selected features back to DataFrame to retain feature names\n",
    "    selected_features = X_train_scaled_df.columns[selector.get_support()]\n",
    "    X_train_selected_df = pd.DataFrame(X_train_selected, columns=selected_features)\n",
    "    X_valid_selected_df = pd.DataFrame(X_valid_selected, columns=selected_features)\n",
    "\n",
    "    # Train LGBMRegressor with selected features\n",
    "    model2 = LGBMRegressor(\n",
    "        learning_rate=0.045,\n",
    "        max_depth=9,\n",
    "        min_child_samples=5,\n",
    "        n_estimators=250,\n",
    "        num_leaves=15,\n",
    "        random_state=42,\n",
    "        verbose=-1\n",
    "    )\n",
    "    model2.fit(X_train_selected_df, y_train)\n",
    "\n",
    "    # Make predictions\n",
    "    y_preds = model2.predict(X_valid_selected_df)\n",
    "\n",
    "    # Compute R¬≤ score\n",
    "    r2 = r2_score(y_valid, y_preds)\n",
    "    r2_scores.append(r2)\n",
    "\n",
    "# Find the optimal k value\n",
    "optimal_k = k_values[np.argmax(r2_scores)]\n",
    "best_r2 = max(r2_scores)\n",
    "\n",
    "# Print optimal k value with highest R¬≤\n",
    "print(f\"Optimal k: {optimal_k} with Highest R¬≤: {best_r2:.4f}\")\n",
    "\n",
    "# Plot R¬≤ Scores vs. k Values\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(k_values, r2_scores, marker='o', linestyle='-', color='lightpink')\n",
    "plt.xlabel(\"Number of Features (k)\", color=\"black\")\n",
    "plt.ylabel(\"R¬≤ Score\", color=\"black\")\n",
    "plt.title(\"Optimal k for SelectKBest Feature Selection\", color=\"black\")\n",
    "plt.gca().set_facecolor('white')  \n",
    "plt.tick_params(axis='both', colors='black')  \n",
    "\n",
    "# Define save path in the specified folder\n",
    "save_path = os.path.join(\"..\", \"notebooks\", \"Plots\", \"optimalkvalue.png\")\n",
    "\n",
    "# Save figure as PNG in the correct directory\n",
    "plt.savefig(save_path, dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "# Success message\n",
    "print(\"Plot saved as optimalkvalue.png\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "üí™üèª Plotting of the data with the optimized machine leaning parameters of this model!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# =============================\n",
    "# Feature Selection with SelectKBest (k=142)\n",
    "# =============================\n",
    "\n",
    "k = 142  # Optimized number of best features for LGBMRegressor\n",
    "selector = SelectKBest(score_func=f_regression, k=k)\n",
    "X_train_selected = selector.fit_transform(X_train_scaled_df, y_train)\n",
    "X_valid_selected = selector.transform(X_valid_scaled_df)\n",
    "\n",
    "# Convert selected features back to DataFrame to retain feature names\n",
    "selected_features = X_train_scaled_df.columns[selector.get_support()]\n",
    "X_train_selected_df = pd.DataFrame(X_train_selected, columns=selected_features)\n",
    "X_valid_selected_df = pd.DataFrame(X_valid_selected, columns=selected_features)\n",
    "\n",
    "# =============================\n",
    "# Train LGBMRegressor with Optimized Parameters\n",
    "# =============================\n",
    "\n",
    "# Define the best parameters found from GridSearchCV\n",
    "best_params = {\n",
    "    'learning_rate': 0.045,\n",
    "    'max_depth': 9,\n",
    "    'min_child_samples': 5,\n",
    "    'n_estimators': 250,\n",
    "    'num_leaves': 15\n",
    "}\n",
    "\n",
    "# Initialize the model with optimized parameters\n",
    "model2 = LGBMRegressor(\n",
    "    learning_rate=best_params['learning_rate'],\n",
    "    max_depth=best_params['max_depth'],\n",
    "    min_child_samples=best_params['min_child_samples'],\n",
    "    n_estimators=best_params['n_estimators'],\n",
    "    num_leaves=best_params['num_leaves'],\n",
    "    random_state=42,\n",
    "    verbose=-1\n",
    ")\n",
    "\n",
    "# Train the model on the selected features\n",
    "model2.fit(X_train_selected_df, y_train)\n",
    "\n",
    "# Make predictions on the validation set\n",
    "y_preds = model2.predict(X_valid_selected_df)\n",
    "\n",
    "# =============================\n",
    "# Define Function for Plotting\n",
    "# =============================\n",
    "\n",
    "\n",
    "\n",
    "# Save figure\n",
    "plt.savefig(save_path, dpi=300, bbox_inches='tight')\n",
    "    \n",
    "\n",
    "\n",
    "# =============================\n",
    "# Plot and Save Results\n",
    "# =============================\n",
    "\n",
    "# Define save path in the specified folder\n",
    "save_path = os.path.join(\"..\", \"notebooks\", \"Plots\", \"validationoptimized_LGBMRegressor.png\")\n",
    "\n",
    "# Plot validation results and display the plot\n",
    "plot_data(y_valid, y_preds, \"Validation Optimized Data - LGBMRegressor\")\n",
    "\n",
    "# Success message\n",
    "print(\"Plot saved as validationoptimized_LGBMRegressor.png\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "‚ö°Ô∏è Computation of statistics for this model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Compute mean error\n",
    "mean_error2 = np.mean(y_preds - y_valid)\n",
    "\n",
    "# Compute variance of the error\n",
    "variance_error2 = np.var(y_preds - y_valid)\n",
    "\n",
    "# Compute mean predicted value\n",
    "mean_predicted2 = np.mean(y_preds)\n",
    "\n",
    "# Compute Mean Squared Error (MSE)\n",
    "mse2 = mean_squared_error(y_valid, y_preds)\n",
    "\n",
    "# Compute R¬≤ score\n",
    "r2_2 = r2_score(y_valid, y_preds)\n",
    "\n",
    "# Compute 95% confidence interval for predictions\n",
    "confidence2 = 0.95\n",
    "n = len(y_preds)\n",
    "std_err2 = stats.sem(y_preds - y_valid)\n",
    "margin_of_error2 = std_err2 * stats.t.ppf((1 + confidence2) / 2., n-1)\n",
    "confidence_interval2 = (mean_error2 - margin_of_error2, mean_error2 + margin_of_error2)\n",
    "\n",
    "# Print computed metrics\n",
    "print(f\"Mean Error: {mean_error2:.4f}\")\n",
    "print(f\"Variance of Error: {variance_error2:.4f}\")\n",
    "print(f\"Mean Predicted Value: {mean_predicted2:.4f}\")\n",
    "print(f\"MSE: {mse2:.4f}\")\n",
    "print(f\"R¬≤: {r2_2:.4f}\")\n",
    "print(f\"95% Confidence Interval: {confidence_interval2}\")\n",
    "\n",
    "# Create a DataFrame for storing the results\n",
    "stats_df = pd.DataFrame({\n",
    "    \"Metric\": [\"Mean Error\", \"Variance\", \"Mean Predicted Value\", \"MSE\", \"R¬≤\", \"95% CI Lower\", \"95% CI Upper\"],\n",
    "    \"Value\": [mean_error2, variance_error2, mean_predicted2, mse2, r2_2, confidence_interval2[0], confidence_interval2[1]]\n",
    "})\n",
    "\n",
    "\n",
    "save_path = os.path.join(\"..\", \"data\",\"LGBM_regressor_stats.csv\")\n",
    "stats_df.to_csv(save_path, index=False)\n",
    "print(f\"Table saved at: {save_path}\")\n",
    "\n",
    "# Display as an interactive HTML table\n",
    "ipd.display(ipd.HTML(stats_df.to_html(classes='table table-striped', escape=False)))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üßê *Comparison of the two machine learning models*\n",
    "\n",
    "When evaluating the machine learning models for pKa prediction, R¬≤ (coefficient of determination) and RMSE (Root Mean Squared Error) are two key metrics used to assess model performance. R¬≤ measures how well the model explains the variance in the experimental pKa values, ranging from 0 (no predictive power) to 1 (perfect prediction). RMSE quantifies the average difference between predicted and actual values, with lower RMSE indicating more accurate predictions.\n",
    "\n",
    "Initially, ExtraTreesRegressor and LGBMRegressor emerged as the top performers based on their R¬≤ scores. However, the final figures demonstrate that both models benefited from hyperparameter tuning and the optimization of k, the number of most relevant descriptors selected using SelectKBest. After optimization, LGBMRegressor achieved the best performance with an R¬≤ of 0.68 and an RMSE of 1.37, while ExtraTreesRegressor followed closely with an R¬≤ of 0.67 and an RMSE of 1.39. The scatter plots confirm strong correlations between predicted and experimental pKa values, with the optimized models producing more accurate and reliable predictions. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Define file paths\n",
    "path1 = os.path.join(\"..\", \"data\", \"ExtraTreesRegressor_stats.csv\")\n",
    "path2 = os.path.join(\"..\", \"data\", \"LGBM_regressor_stats.csv\")\n",
    "\n",
    "# Load data\n",
    "extra_trees_df = pd.read_csv(path1)\n",
    "lgbm_df = pd.read_csv(path2)\n",
    "\n",
    "# Merge data for comparison\n",
    "comparison_df = extra_trees_df.merge(lgbm_df, on=\"Metric\", suffixes=(\"_ExtraTrees\", \"_LGBM\"))\n",
    "\n",
    "# Define metrics and values\n",
    "metrics = comparison_df[\"Metric\"]\n",
    "values_extra_trees = comparison_df[\"Value_ExtraTrees\"]\n",
    "values_lgbm = comparison_df[\"Value_LGBM\"]\n",
    "\n",
    "# Set up subplot grid (4 columns, 3 rows)\n",
    "n_cols = 4\n",
    "n_rows = int(np.ceil(len(metrics) / n_cols))\n",
    "fig, axes = plt.subplots(n_rows, n_cols, figsize=(n_cols * 4, n_rows * 3), sharey=False, facecolor='white')\n",
    "axes = axes.flatten()\n",
    "\n",
    "for i, metric in enumerate(metrics):\n",
    "    ax = axes[i]\n",
    "    x_center = 0.5  \n",
    "    colors = ['#A6D785', '#FF69B4'] \n",
    "    labels = [\"ExtraTrees\", \"LGBM\"]\n",
    "    values = [values_extra_trees[i], values_lgbm[i]]\n",
    "    \n",
    "    # Set specific y-axis limits for R¬≤\n",
    "    if metric == \"R¬≤\":\n",
    "        y_min, y_max = 0.65, 0.72\n",
    "    else:\n",
    "        \n",
    "        rounded_values = [round(val, 2) for val in values]\n",
    "        y_min, y_max = min(rounded_values) - 0.1, max(rounded_values) + 0.1\n",
    "    ax.set_ylim(y_min, y_max)\n",
    "    \n",
    "    \n",
    "    ax.spines['top'].set_visible(False)\n",
    "    ax.spines['right'].set_visible(False)\n",
    "    ax.spines['bottom'].set_visible(False)\n",
    "    ax.spines['left'].set_visible(True)\n",
    "    ax.spines['left'].set_color('black')\n",
    "    ax.spines['left'].set_linewidth(1)\n",
    "    ax.grid(False)  \n",
    "    \n",
    "    \n",
    "    y_ticks = np.linspace(y_min, y_max, num=5)\n",
    "    ax.set_yticks(y_ticks)\n",
    "    ax.set_yticklabels([f\"{tick:.2f}\" for tick in y_ticks], fontsize=10)\n",
    "    \n",
    "    \n",
    "    for j, val in enumerate(values):\n",
    "        ax.scatter(x_center, val, color=colors[j], s=100)\n",
    "        ax.text(x_center + 0.05, val, f\"{val:.3f}\", ha='left', va='center', fontsize=10, color='black')\n",
    "    \n",
    "    ax.set_xticks([])  \n",
    "    ax.set_ylabel(\"Values\")  \n",
    "    ax.set_title(f\"{metric} Comparison\", pad=20)\n",
    "    ax.set_facecolor('white')  \n",
    "\n",
    "\n",
    "legend_handles = [plt.Line2D([0], [0], marker='o', color='w', markerfacecolor=colors[0], markersize=10, label=\"ExtraTrees\"),\n",
    "                  plt.Line2D([0], [0], marker='o', color='w', markerfacecolor=colors[1], markersize=10, label=\"LGBM\")]\n",
    "fig.legend(handles=legend_handles, loc=\"upper right\", bbox_to_anchor=(1.15, 1.05), frameon=False, markerscale=1.5, fontsize=12, \n",
    "           handletextpad=1, labelspacing=1, borderpad=1)\n",
    "\n",
    "\n",
    "for i in range(len(metrics), len(axes)):\n",
    "    fig.delaxes(axes[i])\n",
    "\n",
    "plt.tight_layout()\n",
    "\n",
    "\n",
    "# Define save path\n",
    "save_path = os.path.join(\"..\", \"notebooks\", \"plots\", \"model_comparison_stats.png\")\n",
    "plt.savefig(save_path, dpi=300, bbox_inches='tight')\n",
    "print(f\"Figure saved at: {save_path}\")\n",
    "plt.show()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The comparative analysis between the ExtraTrees and LGBM models reveals distinct performance characteristics across key evaluation metrics.\n",
    "\n",
    "**Mean Error** : The ExtraTrees model shows a slightly larger absolute error (-0.034) compared to LGBM (-0.005), indicating that LGBM produces predictions that are closer to the expected values on average.\n",
    "\n",
    "**Variance**: The variance is comparable between both models, with ExtraTrees exhibiting 1.921 and LGBM 1.883, suggesting similar levels of model variability.\n",
    "\n",
    "**Mean Predicted Value**: Both models yield nearly identical predicted values, with LGBM slightly higher at 6.783 compared to 6.754 for ExtraTrees.\n",
    "\n",
    "**Mean Squared Error (MSE)**: ExtraTrees has a marginally higher MSE (1.922) compared to LGBM (1.884), suggesting that the LGBM model achieves slightly better prediction accuracy.\n",
    "\n",
    "**R¬≤ Score**: The ExtraTrees model has an R¬≤ of 0.674, while LGBM scores 0.680, reinforcing that both models explain the variance in the data quite well, but LGBM performs slightly better in this regard.\n",
    "\n",
    "**Confidence Intervals**:\n",
    "\n",
    "The 95% CI lower bound is slightly lower for ExtraTrees (-0.275) than for LGBM (-0.244).\n",
    "The 95% CI upper bound is 0.208 for ExtraTrees and 0.234 for LGBM, indicating a slightly wider confidence interval for LGBM.\n",
    "Conclusion\n",
    "While both models exhibit comparable performance, LGBM slightly outperforms ExtraTrees in most metrics, particularly in terms of lower mean error, higher R¬≤, and marginally better MSE. However, the differences are not drastic, and the choice between these models may depend on factors such as computational efficiency and interpretability."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üßÖ *Saving the LGBMRegressor trained model*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Ensure the models directory exists\n",
    "os.makedirs(\"models\", exist_ok=True)\n",
    "\n",
    "# Save the best model\n",
    "with open(\"models/best_pKa_model.pkl\", \"wb\") as file:\n",
    "    pickle.dump(model2, file)\n",
    "\n",
    "print(\"‚úÖ Best model saved as 'models/best_pKa_model.pkl'\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ü©∑ *Results: Usage of this trained machine learning model*\n",
    "As illustrated below, the functionalities provided by the modules of the *pKaPredict* package allow users to input the SMILES string of a target molecule, which is then automatically converted into molecular descriptors. Based on these descriptors, the predicted pKa value is thus computed and displayed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Exact descriptor list used during training\n",
    "descriptor_names = [ 'MaxAbsEStateIndex', 'MaxEStateIndex', 'MinAbsEStateIndex', 'MinEStateIndex', 'SPS', \n",
    "    'NumValenceElectrons', 'MaxPartialCharge', 'MinPartialCharge', 'MaxAbsPartialCharge', 'MinAbsPartialCharge', \n",
    "    'FpDensityMorgan1', 'FpDensityMorgan2', 'FpDensityMorgan3', 'BCUT2D_MWHI', 'BCUT2D_MWLOW', \n",
    "    'BCUT2D_CHGLO', 'BCUT2D_LOGPLOW', 'BCUT2D_MRHI', 'BalabanJ', 'BertzCT', 'Chi0', 'Chi0n', 'Chi1', \n",
    "    'Chi1n', 'Chi3v', 'Chi4v', 'HallKierAlpha', 'Kappa1', 'Kappa2', 'Kappa3', 'LabuteASA', 'PEOE_VSA1', \n",
    "    'PEOE_VSA12', 'PEOE_VSA13', 'PEOE_VSA14', 'PEOE_VSA3', 'PEOE_VSA4', 'PEOE_VSA5', 'PEOE_VSA6', \n",
    "    'PEOE_VSA7', 'PEOE_VSA8', 'PEOE_VSA9', 'SMR_VSA1', 'SMR_VSA10', 'SMR_VSA2', 'SMR_VSA3', 'SMR_VSA4', \n",
    "    'SMR_VSA5', 'SMR_VSA6', 'SMR_VSA7', 'SMR_VSA9', 'SlogP_VSA1', 'SlogP_VSA10', 'SlogP_VSA11', \n",
    "    'SlogP_VSA12', 'SlogP_VSA2', 'SlogP_VSA3', 'SlogP_VSA4', 'SlogP_VSA5', 'SlogP_VSA6', 'SlogP_VSA7', \n",
    "    'SlogP_VSA8', 'TPSA', 'EState_VSA1', 'EState_VSA10', 'EState_VSA2', 'EState_VSA3', 'EState_VSA4', \n",
    "    'EState_VSA5', 'EState_VSA6', 'EState_VSA7', 'EState_VSA8', 'EState_VSA9', 'VSA_EState1', \n",
    "    'VSA_EState10', 'VSA_EState2', 'VSA_EState3', 'VSA_EState4', 'VSA_EState6', 'VSA_EState7', \n",
    "    'VSA_EState8', 'VSA_EState9', 'FractionCSP3', 'NHOHCount', 'NOCount', 'NumAliphaticHeterocycles', \n",
    "    'NumAliphaticRings', 'NumAromaticHeterocycles', 'NumAromaticRings', 'NumBridgeheadAtoms', \n",
    "    'NumHAcceptors', 'NumHeteroatoms', 'NumHeterocycles', 'NumRotatableBonds', 'NumSaturatedHeterocycles', \n",
    "    'NumSaturatedRings', 'Phi', 'MolMR', 'fr_Al_COO', 'fr_ArN', 'fr_Ar_COO', 'fr_Ar_N', 'fr_Ar_OH', \n",
    "    'fr_COO', 'fr_COO2', 'fr_C_O', 'fr_C_O_noCOO', 'fr_C_S', 'fr_HOCCN', 'fr_Imine', 'fr_NH0', \n",
    "    'fr_NH1', 'fr_NH2', 'fr_N_O', 'fr_Ndealkylation1', 'fr_Ndealkylation2', 'fr_alkyl_halide', \n",
    "    'fr_allylic_oxid', 'fr_amidine', 'fr_aniline', 'fr_aryl_methyl', 'fr_azide', 'fr_azo', 'fr_ester', \n",
    "    'fr_ether', 'fr_guanido', 'fr_halogen', 'fr_imidazole', 'fr_lactam', 'fr_methoxy', 'fr_nitrile', \n",
    "    'fr_nitroso', 'fr_phenol', 'fr_phenol_noOrthoHbond', 'fr_piperdine', 'fr_pyridine', 'fr_quatN', \n",
    "    'fr_sulfide', 'fr_sulfonamd', 'fr_sulfone', 'fr_tetrazole', 'fr_thiazole'\n",
    "]\n",
    "\n",
    "from pkapredict.predict_pKa import predict_pKa \n",
    "from pkapredict.load_model import load_model\n",
    "def main():\n",
    "    print(\"üî¨ Welcome to the pKa Predictor!\")\n",
    "    smiles = input(\"üëâ Enter a SMILES string (e.g., [NH4+]): \").strip()\n",
    "\n",
    "    try:\n",
    "        model = load_model()\n",
    "        predicted_pKa = predict_pKa(smiles, model, descriptor_names)\n",
    "        print(f\"\\n‚úÖ Predicted pKa for {smiles}: {predicted_pKa:.2f}\")\n",
    "    except ValueError as ve:\n",
    "        print(str(ve))\n",
    "    except FileNotFoundError:\n",
    "        print(\"‚ùå Model file not found. Please check the path.\")\n",
    "    except Exception as e:\n",
    "        print(f\"‚ö†Ô∏è Unexpected error: {e}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üí¨ *Discussion*\n",
    "\n",
    "The *pKaPredict* package demonstrates promising predictive performance, with the LGBMRegressor model achieving an R¬≤ value of 0.68 and a root mean squared error (RMSE) of 1.37 on the validation set. These metrics suggest that while the model captures a substantial portion of the variance in the data, notable deviations from experimental values remain. An R¬≤ value of 0.68 indicates that approximately 68% of the variance in the experimental pKa measurements is accounted for by the model, reflecting a moderate degree of correlation. However, this also implies that around 32% of the variance remains unexplained, suggesting room for improvement. Additionally, the root mean squared error (RMSE) of 1.37 indicates that, on average, the predicted pKa values deviate from the true values by more than one unit which is a non-negligible margin. The dispersion of data points around the regression line further highlights that certain molecules, particularly those with higher pKa values, are subject to more pronounced over- or under-predictions, pointing to potential challenges in generalizing across the full range of chemical structures present in the dataset.\n",
    "\n",
    "Another factor that can account for the limitations of the package in predictive accuracy is training data imbalance. The distribution of pKa values in the dataset is not uniform. Certain pKa ranges, particularly around values of 4‚Äì5 and 9‚Äì10‚Äîare more densely represented than others. This imbalance can lead the model to be biased toward frequently occurring ranges, reducing its accuracy for less represented values.\n",
    "\n",
    "Moreover, although the LGBMRegressor is a powerful gradient boosting algorithm, it is still a data-driven model. Its performance is heavily dependent on the diversity and quality of the training set. In cases where the chemical structures or functional groups of the test molecules differ significantly from those in the training set, the model may struggle to generalize and produce reliable predictions.\n",
    "\n",
    "In summary, while the *pKaPredict* package provides a functional and efficient pipeline for estimating pKa values, the accuracy of its predictions is constrained by the training data imbalance and the data-driven nature of the model. Future improvements may include enriching the training dataset with more diverse molecular structures, and exploring ensemble modeling approaches to enhance predictive robustness."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pKaPredict",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
