{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Welcome to our project report! ‚ú®üß™**\n",
    "\n",
    "## üåé *Introduction*\n",
    "\n",
    "The pKaPredict package is a cheminformatics tool designed to estimate the acid dissociation constant (pKa) of chemical compounds based on their molecular structure. The [pKa](https://chemistrytalk.org/what-is-pka/) value is a fundamental physicochemical property that indicates the strength of an acid in an aqueous solution. Specifically, it represents the pH at which a molecule exists in equilibrium between its protonated and deprotonated forms. It is defined as the negative logarithm (base 10) of a compound‚Äôs acid dissociation constant in water (Ka). Accurate knowledge of pKa is critical in [fields](https://www.pion-inc.com/blog/what-is-pka-and-how-is-it-used-in-drug-development) such as drug development, environmental chemistry, and molecular biology, where ionization affects solubility, permeability, binding affinity, and reactivity. <br>\n",
    "\n",
    "*pKaPredict* leverages computational chemistry and machine learning to deliver fast and reproducible predictions of pKa values. The pipeline begins by converting SMILES (Simplified Molecular Input Line Entry System) strings into molecular descriptors using the RDKit cheminformatics library. These descriptors encode key structural and electronic features of molecules and serve as input variables for the predictive model. <br>\n",
    "\n",
    "The descriptor generation process is handled by the **smiles_to_rdkit_descriptors** function, which transforms SMILES strings into a comprehensive set of numerical descriptors. These include 2D physicochemical properties, topological indices, and fragment-based counts. <br>\n",
    "\n",
    "In the next stage, the pre-trained regression model is loaded using the load_model function, and pKa predictions are generated through the **predict_pKa** function. The model was trained on a dataset containing molecules with experimentally determined pKa values ranging approximately from 2 to 12. These molecules were first converted into molecular descriptors via the **RDKit_descriptors** function. Among the models tested from the LazyPredict library, the LGBMRegressor yielded the best performance and was selected as the final model. <br>\n",
    "\n",
    "Prior to model training, the dataset was cleaned by removing duplicates and missing values using the **clean_and_visualize_pka** function. Moreover, supplementary tools such as the **plot_data** enable the visualization of predicted versus experimental values and generate graphical summaries to support model evaluation and interpretation. <br>\n",
    "\n",
    "The central component of the package is the **predict_pKa** function, which integrates the aforementioned steps into a streamlined workflow, enabling users to perform end-to-end pKa predictions from molecular input to final output.\n",
    "\n",
    "\n",
    "## üöÄ *Overview*\n",
    "ü§Ø Acquiring Dataset <br>\n",
    "üßπ Cleaning Dataset <br>\n",
    "üõü Saving the cleaned data to a csv file <br>\n",
    "ü§ì Computation of RDKit Molecular Descriptors <br>\n",
    "üí° Formatting the dataset for machine learning <br>\n",
    "üïπÔ∏è Machine learning model selection <br>\n",
    "üå≤ Machine learning model ü•á: ExtraTreesRegressor <br>\n",
    "ü§ñ Machine learning model ü•à : LGBMRegressor <br>\n",
    "üßê Comparison of the two machine learning models <br>\n",
    "üßÖ Saving the LGBMRegressor trained model <br>\n",
    "ü©∑ Results: usage of this trained machine learning model <br>\n",
    "üí¨ Discussion\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Import statements**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standard library\n",
    "import os\n",
    "import time\n",
    "import pickle\n",
    "from pathlib import Path\n",
    "\n",
    "# Data handling\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Plotting and visualization\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import seaborn as sn  # Included both in case both are used differently\n",
    "import matplotlib.patches as mpatches\n",
    "\n",
    "# Web and display\n",
    "import requests\n",
    "import IPython.display as ipd\n",
    "\n",
    "# Chemistry\n",
    "from rdkit import Chem\n",
    "from rdkit.Chem import AllChem, Descriptors\n",
    "from rdkit.ML.Descriptors import MoleculeDescriptors\n",
    "from rdkit.ML.Descriptors.MoleculeDescriptors import MolecularDescriptorCalculator\n",
    "\n",
    "# Machine learning and preprocessing\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.ensemble import ExtraTreesRegressor\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "from sklearn.feature_selection import SelectKBest, f_regression\n",
    "\n",
    "# LightGBM\n",
    "from lightgbm import LGBMRegressor\n",
    "\n",
    "# LazyPredict\n",
    "from lazypredict.Supervised import LazyRegressor\n",
    "\n",
    "# Statistics\n",
    "from scipy import stats\n",
    "\n",
    "# Progress bar\n",
    "from tqdm import tqdm\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üßπ *Acquiring and Cleaning the Dataset*\n",
    "In a first step, the training [pKa dataset](https://github.com/cbio3lab/pKa/blob/main/Data/test_acids_bases_descfinal_nozwitterions.csv) is acquired from cbio3lab's repository, initially extracted from the Harvard [dataverse](https://dataverse.harvard.edu/dataset.xhtml?persistentId=doi:10.7910/DVN/6A67L9). \n",
    "\n",
    "\n",
    "‚úÖ Downloads the data directly into the working directory <br>\n",
    "‚úÖ Opens the file and verifies its existence as well as displays a preview of the latter <br>\n",
    "‚úÖ Prints initial dataset shape <br> \n",
    "‚úÖ Counts and removes missing values (NaN) and duplicates <br>\n",
    "‚úÖ Prints final dataset shape after cleaning  <br>\n",
    "‚úÖ Generates a histogram to visualize pKa value distribution  <br>\n",
    "‚úÖ Saves the cleaned dataset as a CSV file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ File downloaded successfully: /Users/anastasiafloris/Desktop/pKaPredict/pKaPredict/data/pkadatasetRAWDATA.csv\n",
      "üìÇ Current Directory: /Users/anastasiafloris/Desktop/pKaPredict/pKaPredict/notebooks\n",
      "‚úÖ Dataset file found. Previewing contents...\n",
      "\n",
      "Compound,set,Smiles,pka,prot_smiles,deprot_smiles,acid_base_type,acid_base_string,prot_charge,deprot\n",
      "\n",
      "‚úÖ Dataset successfully loaded. Preview:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Compound</th>\n",
       "      <th>set</th>\n",
       "      <th>Smiles</th>\n",
       "      <th>pka</th>\n",
       "      <th>prot_smiles</th>\n",
       "      <th>deprot_smiles</th>\n",
       "      <th>acid_base_type</th>\n",
       "      <th>acid_base_string</th>\n",
       "      <th>prot_charge</th>\n",
       "      <th>deprot_charge</th>\n",
       "      <th>CH_strength</th>\n",
       "      <th>XH_strength</th>\n",
       "      <th>HBA_strength</th>\n",
       "      <th>Hyd_Apolar</th>\n",
       "      <th>Hyd_Polar</th>\n",
       "      <th>Hyd</th>\n",
       "      <th>CCCC</th>\n",
       "      <th>NCCC</th>\n",
       "      <th>CCNC</th>\n",
       "      <th>CCCO</th>\n",
       "      <th>CCNN</th>\n",
       "      <th>NCCN</th>\n",
       "      <th>CNNC</th>\n",
       "      <th>CNCN</th>\n",
       "      <th>NCCO</th>\n",
       "      <th>CNCO</th>\n",
       "      <th>CNNN</th>\n",
       "      <th>NCNN</th>\n",
       "      <th>OCNN</th>\n",
       "      <th>CCSO</th>\n",
       "      <th>NCSO</th>\n",
       "      <th>nA</th>\n",
       "      <th>nR</th>\n",
       "      <th>nN</th>\n",
       "      <th>nD</th>\n",
       "      <th>nC</th>\n",
       "      <th>nF</th>\n",
       "      <th>nQ</th>\n",
       "      <th>nE</th>\n",
       "      <th>nG</th>\n",
       "      <th>...</th>\n",
       "      <th>VC.5</th>\n",
       "      <th>VC.6</th>\n",
       "      <th>SCH.3</th>\n",
       "      <th>SCH.4</th>\n",
       "      <th>SCH.5</th>\n",
       "      <th>SCH.6</th>\n",
       "      <th>SCH.7</th>\n",
       "      <th>VCH.3</th>\n",
       "      <th>VCH.4</th>\n",
       "      <th>VCH.5</th>\n",
       "      <th>VCH.6</th>\n",
       "      <th>VCH.7</th>\n",
       "      <th>C1SP1</th>\n",
       "      <th>C2SP1</th>\n",
       "      <th>C1SP2</th>\n",
       "      <th>C2SP2</th>\n",
       "      <th>C3SP2</th>\n",
       "      <th>C1SP3</th>\n",
       "      <th>C2SP3</th>\n",
       "      <th>C3SP3</th>\n",
       "      <th>C4SP3</th>\n",
       "      <th>ATSp1</th>\n",
       "      <th>ATSp2</th>\n",
       "      <th>ATSp3</th>\n",
       "      <th>ATSp4</th>\n",
       "      <th>ATSp5</th>\n",
       "      <th>ATSm1</th>\n",
       "      <th>ATSm2</th>\n",
       "      <th>ATSm3</th>\n",
       "      <th>ATSm4</th>\n",
       "      <th>ATSm5</th>\n",
       "      <th>ATSc1</th>\n",
       "      <th>ATSc2</th>\n",
       "      <th>ATSc3</th>\n",
       "      <th>ATSc4</th>\n",
       "      <th>ATSc5</th>\n",
       "      <th>nHBDon</th>\n",
       "      <th>nHBAcc</th>\n",
       "      <th>bpol</th>\n",
       "      <th>apol</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>4</td>\n",
       "      <td>test</td>\n",
       "      <td>Brc1ccc(-c2nn[nH]n2)cc1</td>\n",
       "      <td>3.73</td>\n",
       "      <td>Brc1ccc(-c2nn[nH]n2)cc1</td>\n",
       "      <td>Brc1ccc(-c2nn[n-]n2)cc1</td>\n",
       "      <td>acidic</td>\n",
       "      <td>A</td>\n",
       "      <td>0</td>\n",
       "      <td>-1</td>\n",
       "      <td>3.04</td>\n",
       "      <td>1.01</td>\n",
       "      <td>2.05</td>\n",
       "      <td>-1.65</td>\n",
       "      <td>-9.13</td>\n",
       "      <td>-8.50</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.02</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.14</td>\n",
       "      <td>0.17</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.02</td>\n",
       "      <td>0.04</td>\n",
       "      <td>0.08</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>714.50</td>\n",
       "      <td>800.29</td>\n",
       "      <td>1024.37</td>\n",
       "      <td>793.66</td>\n",
       "      <td>540.99</td>\n",
       "      <td>56.70</td>\n",
       "      <td>20.06</td>\n",
       "      <td>30.05</td>\n",
       "      <td>25.30</td>\n",
       "      <td>16.98</td>\n",
       "      <td>0.09</td>\n",
       "      <td>-0.05</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>7.42</td>\n",
       "      <td>23.10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>6</td>\n",
       "      <td>test</td>\n",
       "      <td>Brc1ccc(C2NCCS2)cc1</td>\n",
       "      <td>5.05</td>\n",
       "      <td>Brc1ccc(C2NCCS2)cc1</td>\n",
       "      <td>Brc1ccc(C2[NH2+]CCS2)cc1</td>\n",
       "      <td>basic</td>\n",
       "      <td>B</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>4.22</td>\n",
       "      <td>0.64</td>\n",
       "      <td>1.71</td>\n",
       "      <td>0.11</td>\n",
       "      <td>-8.59</td>\n",
       "      <td>-8.49</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.06</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.14</td>\n",
       "      <td>0.17</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.18</td>\n",
       "      <td>0.12</td>\n",
       "      <td>0.17</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>986.12</td>\n",
       "      <td>1111.22</td>\n",
       "      <td>1429.49</td>\n",
       "      <td>1070.47</td>\n",
       "      <td>786.77</td>\n",
       "      <td>61.75</td>\n",
       "      <td>22.32</td>\n",
       "      <td>34.09</td>\n",
       "      <td>27.98</td>\n",
       "      <td>19.32</td>\n",
       "      <td>0.11</td>\n",
       "      <td>-0.07</td>\n",
       "      <td>0.02</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>15.16</td>\n",
       "      <td>29.56</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>7</td>\n",
       "      <td>test</td>\n",
       "      <td>Brc1ccc(Cc2c3ccccc3nc3ccccc23)cc1</td>\n",
       "      <td>7.00</td>\n",
       "      <td>Brc1ccc(Cc2c3ccccc3[nH+]c3ccccc23)cc1</td>\n",
       "      <td>Brc1ccc(Cc2c3ccccc3nc3ccccc23)cc1</td>\n",
       "      <td>basic</td>\n",
       "      <td>B</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>6.98</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1.17</td>\n",
       "      <td>-3.98</td>\n",
       "      <td>-6.69</td>\n",
       "      <td>-10.66</td>\n",
       "      <td>9</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.09</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.30</td>\n",
       "      <td>0.52</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.10</td>\n",
       "      <td>0.17</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>15</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1786.71</td>\n",
       "      <td>2120.44</td>\n",
       "      <td>3000.22</td>\n",
       "      <td>3087.64</td>\n",
       "      <td>2611.27</td>\n",
       "      <td>65.62</td>\n",
       "      <td>30.99</td>\n",
       "      <td>46.97</td>\n",
       "      <td>48.14</td>\n",
       "      <td>37.15</td>\n",
       "      <td>0.09</td>\n",
       "      <td>-0.04</td>\n",
       "      <td>-0.01</td>\n",
       "      <td>0.02</td>\n",
       "      <td>-0.01</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>17.91</td>\n",
       "      <td>48.69</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>18</td>\n",
       "      <td>test</td>\n",
       "      <td>Brc1cccc(Br)c1N(C1=NCCN1)C1CCCCC1</td>\n",
       "      <td>11.30</td>\n",
       "      <td>Brc1cccc(Br)c1N(C1=[NH+]CCN1)C1CCCCC1</td>\n",
       "      <td>Brc1cccc(Br)c1N(C1=NCCN1)C1CCCCC1</td>\n",
       "      <td>basic</td>\n",
       "      <td>B</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>7.44</td>\n",
       "      <td>0.79</td>\n",
       "      <td>3.17</td>\n",
       "      <td>-0.11</td>\n",
       "      <td>-15.17</td>\n",
       "      <td>-13.03</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.19</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.14</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0.33</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.06</td>\n",
       "      <td>0.15</td>\n",
       "      <td>0.18</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1844.09</td>\n",
       "      <td>2174.76</td>\n",
       "      <td>3036.79</td>\n",
       "      <td>3135.77</td>\n",
       "      <td>3067.07</td>\n",
       "      <td>107.60</td>\n",
       "      <td>34.47</td>\n",
       "      <td>54.69</td>\n",
       "      <td>67.79</td>\n",
       "      <td>111.84</td>\n",
       "      <td>0.24</td>\n",
       "      <td>-0.17</td>\n",
       "      <td>0.10</td>\n",
       "      <td>-0.04</td>\n",
       "      <td>-0.07</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>27.31</td>\n",
       "      <td>48.47</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>21</td>\n",
       "      <td>test</td>\n",
       "      <td>Brc1cccc(Br)c1N(CC1CCCC1)C1=NCCN1</td>\n",
       "      <td>10.90</td>\n",
       "      <td>Brc1cccc(Br)c1N(CC1CCCC1)C1=[NH+]CCN1</td>\n",
       "      <td>Brc1cccc(Br)c1N(CC1CCCC1)C1=NCCN1</td>\n",
       "      <td>basic</td>\n",
       "      <td>B</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>7.29</td>\n",
       "      <td>0.67</td>\n",
       "      <td>3.11</td>\n",
       "      <td>-0.12</td>\n",
       "      <td>-14.85</td>\n",
       "      <td>-12.87</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.16</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.29</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0.34</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.20</td>\n",
       "      <td>0.15</td>\n",
       "      <td>0.18</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1781.49</td>\n",
       "      <td>2087.41</td>\n",
       "      <td>2884.44</td>\n",
       "      <td>2771.76</td>\n",
       "      <td>2746.05</td>\n",
       "      <td>107.60</td>\n",
       "      <td>34.47</td>\n",
       "      <td>54.52</td>\n",
       "      <td>64.79</td>\n",
       "      <td>108.67</td>\n",
       "      <td>0.24</td>\n",
       "      <td>-0.18</td>\n",
       "      <td>0.11</td>\n",
       "      <td>-0.05</td>\n",
       "      <td>-0.07</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>27.31</td>\n",
       "      <td>48.47</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>29</td>\n",
       "      <td>test</td>\n",
       "      <td>Brc1ccccc1-c1nn[nH]n1</td>\n",
       "      <td>3.83</td>\n",
       "      <td>Brc1ccccc1-c1nn[nH]n1</td>\n",
       "      <td>Brc1ccccc1-c1nn[n-]n1</td>\n",
       "      <td>acidic</td>\n",
       "      <td>A</td>\n",
       "      <td>0</td>\n",
       "      <td>-1</td>\n",
       "      <td>2.67</td>\n",
       "      <td>1.01</td>\n",
       "      <td>1.89</td>\n",
       "      <td>-1.65</td>\n",
       "      <td>-8.18</td>\n",
       "      <td>-7.69</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.10</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.14</td>\n",
       "      <td>0.17</td>\n",
       "      <td>0.24</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.02</td>\n",
       "      <td>0.04</td>\n",
       "      <td>0.08</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>750.70</td>\n",
       "      <td>852.05</td>\n",
       "      <td>1110.85</td>\n",
       "      <td>912.21</td>\n",
       "      <td>643.90</td>\n",
       "      <td>56.70</td>\n",
       "      <td>20.06</td>\n",
       "      <td>30.05</td>\n",
       "      <td>31.96</td>\n",
       "      <td>32.50</td>\n",
       "      <td>0.09</td>\n",
       "      <td>-0.04</td>\n",
       "      <td>0.01</td>\n",
       "      <td>-0.04</td>\n",
       "      <td>0.02</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>7.42</td>\n",
       "      <td>23.10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>35</td>\n",
       "      <td>test</td>\n",
       "      <td>Brc1cncc2ccccc12</td>\n",
       "      <td>3.54</td>\n",
       "      <td>Brc1c[nH+]cc2ccccc12</td>\n",
       "      <td>Brc1cncc2ccccc12</td>\n",
       "      <td>basic</td>\n",
       "      <td>B</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>3.67</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1.08</td>\n",
       "      <td>-1.57</td>\n",
       "      <td>-5.54</td>\n",
       "      <td>-6.67</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.11</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.15</td>\n",
       "      <td>0.27</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.05</td>\n",
       "      <td>0.09</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>768.06</td>\n",
       "      <td>877.89</td>\n",
       "      <td>1164.03</td>\n",
       "      <td>1019.74</td>\n",
       "      <td>531.30</td>\n",
       "      <td>54.62</td>\n",
       "      <td>17.99</td>\n",
       "      <td>27.64</td>\n",
       "      <td>33.40</td>\n",
       "      <td>26.29</td>\n",
       "      <td>0.10</td>\n",
       "      <td>-0.05</td>\n",
       "      <td>-0.02</td>\n",
       "      <td>0.03</td>\n",
       "      <td>-0.01</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>9.17</td>\n",
       "      <td>23.99</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>41</td>\n",
       "      <td>test</td>\n",
       "      <td>C#CC(C)(C)N(C)C(C)(C)C#C</td>\n",
       "      <td>6.65</td>\n",
       "      <td>C#CC(C)(C)N(C)C(C)(C)C#C</td>\n",
       "      <td>C#CC(C)(C)[NH+](C)C(C)(C)C#C</td>\n",
       "      <td>basic</td>\n",
       "      <td>B</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>5.17</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.81</td>\n",
       "      <td>0.47</td>\n",
       "      <td>-3.82</td>\n",
       "      <td>-3.36</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.45</td>\n",
       "      <td>0.11</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>717.95</td>\n",
       "      <td>788.18</td>\n",
       "      <td>1090.48</td>\n",
       "      <td>987.38</td>\n",
       "      <td>636.34</td>\n",
       "      <td>12.36</td>\n",
       "      <td>11.50</td>\n",
       "      <td>18.00</td>\n",
       "      <td>18.33</td>\n",
       "      <td>13.00</td>\n",
       "      <td>0.11</td>\n",
       "      <td>-0.07</td>\n",
       "      <td>0.02</td>\n",
       "      <td>0.00</td>\n",
       "      <td>-0.00</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>20.56</td>\n",
       "      <td>31.80</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>52</td>\n",
       "      <td>test</td>\n",
       "      <td>C#CCCCC(=O)O</td>\n",
       "      <td>4.60</td>\n",
       "      <td>C#CCCCC(=O)O</td>\n",
       "      <td>C#CCCCC(=O)[O-]</td>\n",
       "      <td>acidic</td>\n",
       "      <td>A</td>\n",
       "      <td>0</td>\n",
       "      <td>-1</td>\n",
       "      <td>4.11</td>\n",
       "      <td>1.06</td>\n",
       "      <td>1.65</td>\n",
       "      <td>0.10</td>\n",
       "      <td>-8.93</td>\n",
       "      <td>-8.83</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>241.27</td>\n",
       "      <td>227.43</td>\n",
       "      <td>211.54</td>\n",
       "      <td>153.95</td>\n",
       "      <td>109.45</td>\n",
       "      <td>9.55</td>\n",
       "      <td>7.66</td>\n",
       "      <td>8.44</td>\n",
       "      <td>5.66</td>\n",
       "      <td>4.66</td>\n",
       "      <td>0.20</td>\n",
       "      <td>-0.09</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.00</td>\n",
       "      <td>-0.06</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>9.70</td>\n",
       "      <td>17.50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>62</td>\n",
       "      <td>test</td>\n",
       "      <td>C#CCCN</td>\n",
       "      <td>9.23</td>\n",
       "      <td>C#CCC[NH3+]</td>\n",
       "      <td>C#CCCN</td>\n",
       "      <td>basic</td>\n",
       "      <td>B</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1.89</td>\n",
       "      <td>1.05</td>\n",
       "      <td>1.16</td>\n",
       "      <td>0.30</td>\n",
       "      <td>-4.97</td>\n",
       "      <td>-4.68</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>136.47</td>\n",
       "      <td>115.58</td>\n",
       "      <td>83.60</td>\n",
       "      <td>48.47</td>\n",
       "      <td>19.19</td>\n",
       "      <td>5.36</td>\n",
       "      <td>4.17</td>\n",
       "      <td>3.17</td>\n",
       "      <td>2.17</td>\n",
       "      <td>1.17</td>\n",
       "      <td>0.05</td>\n",
       "      <td>-0.01</td>\n",
       "      <td>-0.02</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>6.99</td>\n",
       "      <td>12.81</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10 rows √ó 274 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   Compound   set                             Smiles  ...  nHBAcc  bpol  apol\n",
       "0         4  test            Brc1ccc(-c2nn[nH]n2)cc1  ...       3  7.42 23.10\n",
       "1         6  test                Brc1ccc(C2NCCS2)cc1  ...       1 15.16 29.56\n",
       "2         7  test  Brc1ccc(Cc2c3ccccc3nc3ccccc23)cc1  ...       1 17.91 48.69\n",
       "3        18  test  Brc1cccc(Br)c1N(C1=NCCN1)C1CCCCC1  ...       3 27.31 48.47\n",
       "4        21  test  Brc1cccc(Br)c1N(CC1CCCC1)C1=NCCN1  ...       3 27.31 48.47\n",
       "5        29  test              Brc1ccccc1-c1nn[nH]n1  ...       3  7.42 23.10\n",
       "6        35  test                   Brc1cncc2ccccc12  ...       1  9.17 23.99\n",
       "7        41  test           C#CC(C)(C)N(C)C(C)(C)C#C  ...       1 20.56 31.80\n",
       "8        52  test                       C#CCCCC(=O)O  ...       2  9.70 17.50\n",
       "9        62  test                             C#CCCN  ...       1  6.99 12.81\n",
       "\n",
       "[10 rows x 274 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üîπ Checking dataset information:\n",
      "Initial dataset shape: (1297, 274)\n",
      "\n",
      "Missing values before cleaning:\n",
      "acid_base_type    0\n",
      "Smiles            0\n",
      "pka               0\n",
      "dtype: int64\n",
      "\n",
      "Total duplicate rows removed: 0\n",
      "Dataset shape after NaN and duplicate removal: (1297, 3)\n",
      "üíæ Cleaned dataset saved to: /Users/anastasiafloris/Desktop/pKaPredict/pKaPredict/data/pkadataset_cleaned.csv (shape: (1297, 3))\n"
     ]
    }
   ],
   "source": [
    "from pkapredict.data_preprocessing import download_raw_dataset, preview_data, clean_and_visualize_pka\n",
    "\n",
    "file_path = download_raw_dataset()\n",
    "df = preview_data()\n",
    "cleaned_df = clean_and_visualize_pka(df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "üìä Presentation of the pKa dataset as a histogram:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üìä Generating histogram for pKa distribution...\n",
      "\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 800x500 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìÅ Histogram saved successfully at: ../notebooks/Plots/pKa_distribution_of_the_dataset.png\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Generate histogram for pKa distribution\n",
    "print(\"\\nüìä Generating histogram for pKa distribution...\\n\")\n",
    "\n",
    "# Set white background \n",
    "sns.set_theme(style=\"white\")\n",
    "\n",
    "# Create figure\n",
    "plt.figure(figsize=(8, 5))\n",
    "sns.histplot(data=cleaned_df, x=\"pka\", binwidth=0.5, kde=True, color=\"pink\")  \n",
    "\n",
    "\n",
    "plt.xlabel(\"pKa\", fontsize=12)\n",
    "plt.ylabel(\"Frequency\", fontsize=12)\n",
    "plt.title(\"Distribution of pKa Values\", fontsize=14)\n",
    "plt.grid(False)  \n",
    "\n",
    "# Define save path in the specified folder\n",
    "save_path = os.path.join(\"..\", \"notebooks\", \"Plots\", \"pKa_distribution_of_the_dataset.png\")\n",
    "\n",
    "# Save histogram as PNG\n",
    "plt.savefig(save_path, dpi=300, bbox_inches=\"tight\")\n",
    "\n",
    "# Show plot\n",
    "plt.show()\n",
    "\n",
    "# Success message\n",
    "print(f\"üìÅ Histogram saved successfully at: {save_path}\")\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ü§ì *Computation of RDKit Molecular Descriptors*\n",
    "Molecular descriptors are essential for pKa prediction using machine learning because they provide numerical representations of molecular structures, enabling the model to identify patterns and correlations between molecular features and pKa values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Computing Molecular Descriptors: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1297/1297 [02:36<00:00,  8.27it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Descriptor computation completed.\n",
      "Training data descriptors saved as '../data/Data_pKa_Descriptors.csv'.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from pkapredict.RDkit_descriptors import RDkit_descriptors\n",
    "\n",
    "# Load cleaned data containing SMILES\n",
    "file_path_cleaned = os.path.join(\"..\", \"data\", \"pKadataset_cleaned.csv\")\n",
    "cleaned_data = pd.read_csv(file_path_cleaned)\n",
    "\n",
    "# Compute descriptors\n",
    "Mol_descriptors, desc_names = RDkit_descriptors(cleaned_data['Smiles'])\n",
    "\n",
    "# Create DataFrame with descriptors and add SMILES column\n",
    "df_descriptors = pd.DataFrame(Mol_descriptors, columns=desc_names)\n",
    "df_descriptors.insert(0, \"Smiles\", cleaned_data['Smiles'])  # Insert Smiles as the first column\n",
    "\n",
    "# Save descriptors as CSV file in pkapredict/data\n",
    "save_path = os.path.join(\"..\", \"data\", \"Data_pKa_Descriptors.csv\")\n",
    "df_descriptors.to_csv(save_path, index=False)\n",
    "\n",
    "print(\"Descriptor computation completed.\")\n",
    "print(f\"Training data descriptors saved as '{save_path}'.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üí° *Formatting the dataset for machine learning*\n",
    "This section of the code prepares the dataset for training a machine learning model by splitting it into training and validation sets and standardizing the features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Descriptors DataFrame:\n",
      "                               Smiles  ...  fr_urea\n",
      "0            Brc1ccc(-c2nn[nH]n2)cc1  ...        0\n",
      "1                Brc1ccc(C2NCCS2)cc1  ...        0\n",
      "2  Brc1ccc(Cc2c3ccccc3nc3ccccc23)cc1  ...        0\n",
      "3  Brc1cccc(Br)c1N(C1=NCCN1)C1CCCCC1  ...        0\n",
      "4  Brc1cccc(Br)c1N(CC1CCCC1)C1=NCCN1  ...        0\n",
      "\n",
      "[5 rows x 218 columns]\n",
      "pKa DataFrame:\n",
      "   acid_base_type                             Smiles   pka\n",
      "0         acidic            Brc1ccc(-c2nn[nH]n2)cc1  3.73\n",
      "1          basic                Brc1ccc(C2NCCS2)cc1  5.05\n",
      "2          basic  Brc1ccc(Cc2c3ccccc3nc3ccccc23)cc1  7.00\n",
      "3          basic  Brc1cccc(Br)c1N(C1=NCCN1)C1CCCCC1 11.30\n",
      "4          basic  Brc1cccc(Br)c1N(CC1CCCC1)C1=NCCN1 10.90\n",
      "X columns before standardization: MaxAbsEStateIndex    float64\n",
      "MaxEStateIndex       float64\n",
      "MinAbsEStateIndex    float64\n",
      "MinEStateIndex       float64\n",
      "qed                  float64\n",
      "                      ...   \n",
      "fr_thiazole            int64\n",
      "fr_thiocyan            int64\n",
      "fr_thiophene           int64\n",
      "fr_unbrch_alkane       int64\n",
      "fr_urea                int64\n",
      "Length: 217, dtype: object\n",
      "Training set shape: (1167, 217), Validation set shape: (130, 217)\n",
      "The training and validation sets have been successfully created üéÄ\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Define file paths\n",
    "file_path1 = os.path.join(\"..\", \"data\", \"Data_pKa_Descriptors.csv\")  # Descriptors (X)\n",
    "file_path2 = os.path.join(\"..\", \"data\", \"pKadataset_cleaned.csv\")  # pKa values (y)\n",
    "\n",
    "# Load the datasets\n",
    "df_descriptors = pd.read_csv(file_path1)  # Features (X)\n",
    "df_pKa = pd.read_csv(file_path2)  # Target values (y)\n",
    "\n",
    "# Display first few rows of both datasets\n",
    "print(\"Descriptors DataFrame:\\n\", df_descriptors.head())\n",
    "print(\"pKa DataFrame:\\n\", df_pKa.head())\n",
    "\n",
    "# Ensure the 'Smiles' column is present in both\n",
    "common_column = 'Smiles'\n",
    "\n",
    "# Merge descriptors with pKa values using 'Smiles'\n",
    "df_merged = df_descriptors.merge(df_pKa[['pka', common_column]], on=common_column)\n",
    "\n",
    "# Drop non-numeric columns (Smiles)\n",
    "X = df_merged.drop(columns=['pka', 'Smiles'])  # Drop pKa (target) and Smiles (string)\n",
    "y = df_merged['pka']  # Target variable (pKa values)\n",
    "\n",
    "# Verify all columns are numeric\n",
    "print(\"X columns before standardization:\", X.dtypes)\n",
    "\n",
    "# Split data into training (90%) and validation (10%) sets\n",
    "X_train, X_valid, y_train, y_valid = train_test_split(X, y, test_size=0.1, random_state=42)\n",
    "\n",
    "# Standardize the features\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_valid_scaled = scaler.transform(X_valid)\n",
    "\n",
    "# Print shape to confirm data processing\n",
    "print(f\"Training set shape: {X_train.shape}, Validation set shape: {X_valid.shape}\")\n",
    "\n",
    "# Success message üéÄ\n",
    "print(\"The training and validation sets have been successfully created üéÄ\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üïπÔ∏è *Machine learning model selection*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/42 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 14%|‚ñà‚ñç        | 6/42 [00:12<00:40,  1.12s/it]/Users/anastasiafloris/miniconda3/envs/pKaPredict/lib/python3.13/site-packages/sklearn/linear_model/_coordinate_descent.py:681: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.5856602971741722, tolerance: 0.5781393707956142\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/Users/anastasiafloris/miniconda3/envs/pKaPredict/lib/python3.13/site-packages/sklearn/linear_model/_coordinate_descent.py:681: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.7303638949188098, tolerance: 0.5781393707956142\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/Users/anastasiafloris/miniconda3/envs/pKaPredict/lib/python3.13/site-packages/sklearn/linear_model/_coordinate_descent.py:681: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 1.9148347867833309, tolerance: 0.5781393707956142\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/Users/anastasiafloris/miniconda3/envs/pKaPredict/lib/python3.13/site-packages/sklearn/linear_model/_coordinate_descent.py:681: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 1.7918444594552057, tolerance: 0.5781393707956142\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/Users/anastasiafloris/miniconda3/envs/pKaPredict/lib/python3.13/site-packages/sklearn/linear_model/_coordinate_descent.py:681: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 1.3805777118868718, tolerance: 0.5781393707956142\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/Users/anastasiafloris/miniconda3/envs/pKaPredict/lib/python3.13/site-packages/sklearn/linear_model/_coordinate_descent.py:681: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.9278898085763103, tolerance: 0.5781393707956142\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/Users/anastasiafloris/miniconda3/envs/pKaPredict/lib/python3.13/site-packages/sklearn/linear_model/_coordinate_descent.py:681: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 2.4629217482859076, tolerance: 0.5781393707956142\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/Users/anastasiafloris/miniconda3/envs/pKaPredict/lib/python3.13/site-packages/sklearn/linear_model/_coordinate_descent.py:681: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 5.546349959255394, tolerance: 0.5781393707956142\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/Users/anastasiafloris/miniconda3/envs/pKaPredict/lib/python3.13/site-packages/sklearn/linear_model/_coordinate_descent.py:681: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.7806107571091161, tolerance: 0.5688048969379167\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/Users/anastasiafloris/miniconda3/envs/pKaPredict/lib/python3.13/site-packages/sklearn/linear_model/_coordinate_descent.py:681: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 1.0678342221419825, tolerance: 0.5688048969379167\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/Users/anastasiafloris/miniconda3/envs/pKaPredict/lib/python3.13/site-packages/sklearn/linear_model/_coordinate_descent.py:681: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 1.0026359644998593, tolerance: 0.5688048969379167\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/Users/anastasiafloris/miniconda3/envs/pKaPredict/lib/python3.13/site-packages/sklearn/linear_model/_coordinate_descent.py:681: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.7602610110905061, tolerance: 0.5688048969379167\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/Users/anastasiafloris/miniconda3/envs/pKaPredict/lib/python3.13/site-packages/sklearn/linear_model/_coordinate_descent.py:681: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 1.2411131960911916, tolerance: 0.5688048969379167\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/Users/anastasiafloris/miniconda3/envs/pKaPredict/lib/python3.13/site-packages/sklearn/linear_model/_coordinate_descent.py:681: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 1.5592975187828415, tolerance: 0.5688048969379167\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/Users/anastasiafloris/miniconda3/envs/pKaPredict/lib/python3.13/site-packages/sklearn/linear_model/_coordinate_descent.py:681: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 2.030192308356618, tolerance: 0.5688048969379167\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/Users/anastasiafloris/miniconda3/envs/pKaPredict/lib/python3.13/site-packages/sklearn/linear_model/_coordinate_descent.py:681: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 3.2234383284894648, tolerance: 0.5688048969379167\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/Users/anastasiafloris/miniconda3/envs/pKaPredict/lib/python3.13/site-packages/sklearn/linear_model/_coordinate_descent.py:681: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 3.009186445570549, tolerance: 0.5688048969379167\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/Users/anastasiafloris/miniconda3/envs/pKaPredict/lib/python3.13/site-packages/sklearn/linear_model/_coordinate_descent.py:681: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 1.9539701209012037, tolerance: 0.5688048969379167\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/Users/anastasiafloris/miniconda3/envs/pKaPredict/lib/python3.13/site-packages/sklearn/linear_model/_coordinate_descent.py:681: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.9040043584077466, tolerance: 0.5600499873855507\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/Users/anastasiafloris/miniconda3/envs/pKaPredict/lib/python3.13/site-packages/sklearn/linear_model/_coordinate_descent.py:681: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 1.3138280361531542, tolerance: 0.5600499873855507\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/Users/anastasiafloris/miniconda3/envs/pKaPredict/lib/python3.13/site-packages/sklearn/linear_model/_coordinate_descent.py:681: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 1.3624216109246845, tolerance: 0.5600499873855507\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/Users/anastasiafloris/miniconda3/envs/pKaPredict/lib/python3.13/site-packages/sklearn/linear_model/_coordinate_descent.py:681: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 2.471437304761821, tolerance: 0.5600499873855507\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/Users/anastasiafloris/miniconda3/envs/pKaPredict/lib/python3.13/site-packages/sklearn/linear_model/_coordinate_descent.py:681: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 1.0686229178759277, tolerance: 0.5600499873855507\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/Users/anastasiafloris/miniconda3/envs/pKaPredict/lib/python3.13/site-packages/sklearn/linear_model/_coordinate_descent.py:681: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 1.6643548347185515, tolerance: 0.5600499873855507\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/Users/anastasiafloris/miniconda3/envs/pKaPredict/lib/python3.13/site-packages/sklearn/linear_model/_coordinate_descent.py:681: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 2.5414862572451966, tolerance: 0.5600499873855507\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/Users/anastasiafloris/miniconda3/envs/pKaPredict/lib/python3.13/site-packages/sklearn/linear_model/_coordinate_descent.py:681: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 1.9493189306786007, tolerance: 0.5600499873855507\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/Users/anastasiafloris/miniconda3/envs/pKaPredict/lib/python3.13/site-packages/sklearn/linear_model/_coordinate_descent.py:681: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 1.7834604903562195, tolerance: 0.5600499873855507\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/Users/anastasiafloris/miniconda3/envs/pKaPredict/lib/python3.13/site-packages/sklearn/linear_model/_coordinate_descent.py:681: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.6413768116058236, tolerance: 0.5616060484034281\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/Users/anastasiafloris/miniconda3/envs/pKaPredict/lib/python3.13/site-packages/sklearn/linear_model/_coordinate_descent.py:681: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 2.0721230116171228, tolerance: 0.5616060484034281\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/Users/anastasiafloris/miniconda3/envs/pKaPredict/lib/python3.13/site-packages/sklearn/linear_model/_coordinate_descent.py:681: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 1.7092947312219167, tolerance: 0.5616060484034281\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/Users/anastasiafloris/miniconda3/envs/pKaPredict/lib/python3.13/site-packages/sklearn/linear_model/_coordinate_descent.py:681: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 2.285769202699157, tolerance: 0.5616060484034281\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/Users/anastasiafloris/miniconda3/envs/pKaPredict/lib/python3.13/site-packages/sklearn/linear_model/_coordinate_descent.py:681: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 2.1470931153869515, tolerance: 0.5616060484034281\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/Users/anastasiafloris/miniconda3/envs/pKaPredict/lib/python3.13/site-packages/sklearn/linear_model/_coordinate_descent.py:681: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 3.0622017489051814, tolerance: 0.5616060484034281\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/Users/anastasiafloris/miniconda3/envs/pKaPredict/lib/python3.13/site-packages/sklearn/linear_model/_coordinate_descent.py:681: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 2.793607084171299, tolerance: 0.5616060484034281\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/Users/anastasiafloris/miniconda3/envs/pKaPredict/lib/python3.13/site-packages/sklearn/linear_model/_coordinate_descent.py:681: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 3.3991897736909777, tolerance: 0.5616060484034281\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/Users/anastasiafloris/miniconda3/envs/pKaPredict/lib/python3.13/site-packages/sklearn/linear_model/_coordinate_descent.py:681: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 4.449823819711128, tolerance: 0.5616060484034281\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/Users/anastasiafloris/miniconda3/envs/pKaPredict/lib/python3.13/site-packages/sklearn/linear_model/_coordinate_descent.py:681: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 3.72677256405791, tolerance: 0.5616060484034281\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/Users/anastasiafloris/miniconda3/envs/pKaPredict/lib/python3.13/site-packages/sklearn/linear_model/_coordinate_descent.py:681: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 1.0288171776619492, tolerance: 0.5624595212816587\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/Users/anastasiafloris/miniconda3/envs/pKaPredict/lib/python3.13/site-packages/sklearn/linear_model/_coordinate_descent.py:681: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.7110968831946138, tolerance: 0.5624595212816587\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/Users/anastasiafloris/miniconda3/envs/pKaPredict/lib/python3.13/site-packages/sklearn/linear_model/_coordinate_descent.py:681: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.9835429426320843, tolerance: 0.5624595212816587\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/Users/anastasiafloris/miniconda3/envs/pKaPredict/lib/python3.13/site-packages/sklearn/linear_model/_coordinate_descent.py:681: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.9074022348170274, tolerance: 0.5624595212816587\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/Users/anastasiafloris/miniconda3/envs/pKaPredict/lib/python3.13/site-packages/sklearn/linear_model/_coordinate_descent.py:681: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 1.8999731559711108, tolerance: 0.5624595212816587\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      " 31%|‚ñà‚ñà‚ñà       | 13/42 [01:02<03:16,  6.78s/it]/Users/anastasiafloris/miniconda3/envs/pKaPredict/lib/python3.13/site-packages/sklearn/linear_model/_huber.py:343: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. OF ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      " 38%|‚ñà‚ñà‚ñà‚ñä      | 16/42 [01:03<01:11,  2.74s/it]/Users/anastasiafloris/miniconda3/envs/pKaPredict/lib/python3.13/site-packages/sklearn/linear_model/_least_angle.py:723: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 103 iterations, i.e. alpha=1.501e-01, with an active set of 88 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "  warnings.warn(\n",
      "/Users/anastasiafloris/miniconda3/envs/pKaPredict/lib/python3.13/site-packages/sklearn/linear_model/_least_angle.py:723: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 332 iterations, i.e. alpha=7.318e+11, with an active set of 199 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "  warnings.warn(\n",
      "/Users/anastasiafloris/miniconda3/envs/pKaPredict/lib/python3.13/site-packages/sklearn/linear_model/_least_angle.py:723: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 333 iterations, i.e. alpha=6.081e+11, with an active set of 200 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "  warnings.warn(\n",
      " 40%|‚ñà‚ñà‚ñà‚ñà      | 17/42 [01:04<00:55,  2.23s/it]/Users/anastasiafloris/miniconda3/envs/pKaPredict/lib/python3.13/site-packages/sklearn/linear_model/_least_angle.py:723: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 322 iterations, i.e. alpha=4.064e+14, with an active set of 191 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "  warnings.warn(\n",
      "/Users/anastasiafloris/miniconda3/envs/pKaPredict/lib/python3.13/site-packages/sklearn/linear_model/_least_angle.py:723: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 329 iterations, i.e. alpha=1.037e+19, with an active set of 193 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "  warnings.warn(\n",
      "/Users/anastasiafloris/miniconda3/envs/pKaPredict/lib/python3.13/site-packages/sklearn/linear_model/_least_angle.py:723: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 329 iterations, i.e. alpha=9.222e+18, with an active set of 193 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "  warnings.warn(\n",
      "/Users/anastasiafloris/miniconda3/envs/pKaPredict/lib/python3.13/site-packages/sklearn/linear_model/_least_angle.py:723: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 331 iterations, i.e. alpha=6.927e+18, with an active set of 194 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "  warnings.warn(\n",
      "/Users/anastasiafloris/miniconda3/envs/pKaPredict/lib/python3.13/site-packages/sklearn/linear_model/_least_angle.py:723: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 344 iterations, i.e. alpha=1.788e+23, with an active set of 198 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "  warnings.warn(\n",
      "/Users/anastasiafloris/miniconda3/envs/pKaPredict/lib/python3.13/site-packages/sklearn/linear_model/_least_angle.py:723: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 344 iterations, i.e. alpha=1.485e+23, with an active set of 198 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "  warnings.warn(\n",
      " 45%|‚ñà‚ñà‚ñà‚ñà‚ñå     | 19/42 [01:08<00:48,  2.13s/it]/Users/anastasiafloris/miniconda3/envs/pKaPredict/lib/python3.13/site-packages/sklearn/linear_model/_coordinate_descent.py:681: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 1.1354263177986468, tolerance: 0.5781393707956142\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/Users/anastasiafloris/miniconda3/envs/pKaPredict/lib/python3.13/site-packages/sklearn/linear_model/_coordinate_descent.py:681: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 1.4381569000433956, tolerance: 0.5781393707956142\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/Users/anastasiafloris/miniconda3/envs/pKaPredict/lib/python3.13/site-packages/sklearn/linear_model/_coordinate_descent.py:681: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 1.5424735405422325, tolerance: 0.5781393707956142\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/Users/anastasiafloris/miniconda3/envs/pKaPredict/lib/python3.13/site-packages/sklearn/linear_model/_coordinate_descent.py:681: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 1.401679679392828, tolerance: 0.5781393707956142\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/Users/anastasiafloris/miniconda3/envs/pKaPredict/lib/python3.13/site-packages/sklearn/linear_model/_coordinate_descent.py:681: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 4.032725623213992, tolerance: 0.5781393707956142\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/Users/anastasiafloris/miniconda3/envs/pKaPredict/lib/python3.13/site-packages/sklearn/linear_model/_coordinate_descent.py:681: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 9.408180678738972, tolerance: 0.5781393707956142\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/Users/anastasiafloris/miniconda3/envs/pKaPredict/lib/python3.13/site-packages/sklearn/linear_model/_coordinate_descent.py:681: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 19.575570741920046, tolerance: 0.5781393707956142\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/Users/anastasiafloris/miniconda3/envs/pKaPredict/lib/python3.13/site-packages/sklearn/linear_model/_coordinate_descent.py:681: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 28.05734537918397, tolerance: 0.5781393707956142\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/Users/anastasiafloris/miniconda3/envs/pKaPredict/lib/python3.13/site-packages/sklearn/linear_model/_coordinate_descent.py:681: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.6227081815450219, tolerance: 0.5688048969379167\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/Users/anastasiafloris/miniconda3/envs/pKaPredict/lib/python3.13/site-packages/sklearn/linear_model/_coordinate_descent.py:681: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.9299485564401948, tolerance: 0.5688048969379167\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/Users/anastasiafloris/miniconda3/envs/pKaPredict/lib/python3.13/site-packages/sklearn/linear_model/_coordinate_descent.py:681: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 1.0810870827249346, tolerance: 0.5688048969379167\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/Users/anastasiafloris/miniconda3/envs/pKaPredict/lib/python3.13/site-packages/sklearn/linear_model/_coordinate_descent.py:681: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 1.4801485228526872, tolerance: 0.5688048969379167\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/Users/anastasiafloris/miniconda3/envs/pKaPredict/lib/python3.13/site-packages/sklearn/linear_model/_coordinate_descent.py:681: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 2.4539608538200355, tolerance: 0.5688048969379167\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/Users/anastasiafloris/miniconda3/envs/pKaPredict/lib/python3.13/site-packages/sklearn/linear_model/_coordinate_descent.py:681: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 3.940659785722346, tolerance: 0.5688048969379167\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/Users/anastasiafloris/miniconda3/envs/pKaPredict/lib/python3.13/site-packages/sklearn/linear_model/_coordinate_descent.py:681: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 8.328165956943849, tolerance: 0.5688048969379167\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/Users/anastasiafloris/miniconda3/envs/pKaPredict/lib/python3.13/site-packages/sklearn/linear_model/_coordinate_descent.py:681: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 4.589263059920995, tolerance: 0.5688048969379167\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/Users/anastasiafloris/miniconda3/envs/pKaPredict/lib/python3.13/site-packages/sklearn/linear_model/_coordinate_descent.py:681: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 2.373289436474579, tolerance: 0.5688048969379167\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/Users/anastasiafloris/miniconda3/envs/pKaPredict/lib/python3.13/site-packages/sklearn/linear_model/_coordinate_descent.py:681: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 3.6276819187892215, tolerance: 0.5688048969379167\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/Users/anastasiafloris/miniconda3/envs/pKaPredict/lib/python3.13/site-packages/sklearn/linear_model/_coordinate_descent.py:681: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 4.12640893511184, tolerance: 0.5688048969379167\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/Users/anastasiafloris/miniconda3/envs/pKaPredict/lib/python3.13/site-packages/sklearn/linear_model/_coordinate_descent.py:681: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 4.065935107499172, tolerance: 0.5688048969379167\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/Users/anastasiafloris/miniconda3/envs/pKaPredict/lib/python3.13/site-packages/sklearn/linear_model/_coordinate_descent.py:681: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 5.627527614107294, tolerance: 0.5688048969379167\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/Users/anastasiafloris/miniconda3/envs/pKaPredict/lib/python3.13/site-packages/sklearn/linear_model/_coordinate_descent.py:681: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 2.695460272159835, tolerance: 0.5688048969379167\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/Users/anastasiafloris/miniconda3/envs/pKaPredict/lib/python3.13/site-packages/sklearn/linear_model/_coordinate_descent.py:681: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 5.055071416660894, tolerance: 0.5688048969379167\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/Users/anastasiafloris/miniconda3/envs/pKaPredict/lib/python3.13/site-packages/sklearn/linear_model/_coordinate_descent.py:681: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 10.261516924926582, tolerance: 0.5688048969379167\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/Users/anastasiafloris/miniconda3/envs/pKaPredict/lib/python3.13/site-packages/sklearn/linear_model/_coordinate_descent.py:681: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.6285252547743312, tolerance: 0.5600499873855507\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/Users/anastasiafloris/miniconda3/envs/pKaPredict/lib/python3.13/site-packages/sklearn/linear_model/_coordinate_descent.py:681: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.6938167167904794, tolerance: 0.5600499873855507\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/Users/anastasiafloris/miniconda3/envs/pKaPredict/lib/python3.13/site-packages/sklearn/linear_model/_coordinate_descent.py:681: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 1.155836783358609, tolerance: 0.5600499873855507\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/Users/anastasiafloris/miniconda3/envs/pKaPredict/lib/python3.13/site-packages/sklearn/linear_model/_coordinate_descent.py:681: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.6279184362999786, tolerance: 0.5600499873855507\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/Users/anastasiafloris/miniconda3/envs/pKaPredict/lib/python3.13/site-packages/sklearn/linear_model/_coordinate_descent.py:681: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.6515103309629922, tolerance: 0.5600499873855507\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/Users/anastasiafloris/miniconda3/envs/pKaPredict/lib/python3.13/site-packages/sklearn/linear_model/_coordinate_descent.py:681: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 1.4610164389282545, tolerance: 0.5600499873855507\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/Users/anastasiafloris/miniconda3/envs/pKaPredict/lib/python3.13/site-packages/sklearn/linear_model/_coordinate_descent.py:681: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 1.208514487260345, tolerance: 0.5600499873855507\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/Users/anastasiafloris/miniconda3/envs/pKaPredict/lib/python3.13/site-packages/sklearn/linear_model/_coordinate_descent.py:681: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.5721815999668252, tolerance: 0.5600499873855507\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/Users/anastasiafloris/miniconda3/envs/pKaPredict/lib/python3.13/site-packages/sklearn/linear_model/_coordinate_descent.py:681: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 2.8073549230898607, tolerance: 0.5600499873855507\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/Users/anastasiafloris/miniconda3/envs/pKaPredict/lib/python3.13/site-packages/sklearn/linear_model/_coordinate_descent.py:681: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 3.0137886930301647, tolerance: 0.5600499873855507\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/Users/anastasiafloris/miniconda3/envs/pKaPredict/lib/python3.13/site-packages/sklearn/linear_model/_coordinate_descent.py:681: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 3.2408938406015295, tolerance: 0.5600499873855507\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/Users/anastasiafloris/miniconda3/envs/pKaPredict/lib/python3.13/site-packages/sklearn/linear_model/_coordinate_descent.py:681: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 3.5979487042595792, tolerance: 0.5600499873855507\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/Users/anastasiafloris/miniconda3/envs/pKaPredict/lib/python3.13/site-packages/sklearn/linear_model/_coordinate_descent.py:681: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 4.674893005472995, tolerance: 0.5600499873855507\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/Users/anastasiafloris/miniconda3/envs/pKaPredict/lib/python3.13/site-packages/sklearn/linear_model/_coordinate_descent.py:681: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 7.0692505420906855, tolerance: 0.5600499873855507\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/Users/anastasiafloris/miniconda3/envs/pKaPredict/lib/python3.13/site-packages/sklearn/linear_model/_coordinate_descent.py:681: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 8.277826776061374, tolerance: 0.5600499873855507\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/Users/anastasiafloris/miniconda3/envs/pKaPredict/lib/python3.13/site-packages/sklearn/linear_model/_coordinate_descent.py:681: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.7168220423145613, tolerance: 0.5616060484034281\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/Users/anastasiafloris/miniconda3/envs/pKaPredict/lib/python3.13/site-packages/sklearn/linear_model/_coordinate_descent.py:681: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 1.0229942380444754, tolerance: 0.5616060484034281\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/Users/anastasiafloris/miniconda3/envs/pKaPredict/lib/python3.13/site-packages/sklearn/linear_model/_coordinate_descent.py:681: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 1.7066444073527691, tolerance: 0.5616060484034281\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/Users/anastasiafloris/miniconda3/envs/pKaPredict/lib/python3.13/site-packages/sklearn/linear_model/_coordinate_descent.py:681: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 2.353460255370237, tolerance: 0.5616060484034281\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/Users/anastasiafloris/miniconda3/envs/pKaPredict/lib/python3.13/site-packages/sklearn/linear_model/_coordinate_descent.py:681: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 1.664685329067197, tolerance: 0.5616060484034281\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/Users/anastasiafloris/miniconda3/envs/pKaPredict/lib/python3.13/site-packages/sklearn/linear_model/_coordinate_descent.py:681: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 2.737828196589817, tolerance: 0.5616060484034281\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/Users/anastasiafloris/miniconda3/envs/pKaPredict/lib/python3.13/site-packages/sklearn/linear_model/_coordinate_descent.py:681: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 2.934940667398905, tolerance: 0.5616060484034281\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/Users/anastasiafloris/miniconda3/envs/pKaPredict/lib/python3.13/site-packages/sklearn/linear_model/_coordinate_descent.py:681: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 3.9152647971964143, tolerance: 0.5616060484034281\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/Users/anastasiafloris/miniconda3/envs/pKaPredict/lib/python3.13/site-packages/sklearn/linear_model/_coordinate_descent.py:681: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 4.792785809498582, tolerance: 0.5616060484034281\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/Users/anastasiafloris/miniconda3/envs/pKaPredict/lib/python3.13/site-packages/sklearn/linear_model/_coordinate_descent.py:681: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 6.487952441091238, tolerance: 0.5616060484034281\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/Users/anastasiafloris/miniconda3/envs/pKaPredict/lib/python3.13/site-packages/sklearn/linear_model/_coordinate_descent.py:681: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 5.879083982774546, tolerance: 0.5616060484034281\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/Users/anastasiafloris/miniconda3/envs/pKaPredict/lib/python3.13/site-packages/sklearn/linear_model/_coordinate_descent.py:681: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 5.0137267002719454, tolerance: 0.5616060484034281\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/Users/anastasiafloris/miniconda3/envs/pKaPredict/lib/python3.13/site-packages/sklearn/linear_model/_coordinate_descent.py:681: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 5.844745915771455, tolerance: 0.5616060484034281\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/Users/anastasiafloris/miniconda3/envs/pKaPredict/lib/python3.13/site-packages/sklearn/linear_model/_coordinate_descent.py:681: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 9.498864394095563, tolerance: 0.5616060484034281\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/Users/anastasiafloris/miniconda3/envs/pKaPredict/lib/python3.13/site-packages/sklearn/linear_model/_coordinate_descent.py:681: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 11.501274199668615, tolerance: 0.5616060484034281\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/Users/anastasiafloris/miniconda3/envs/pKaPredict/lib/python3.13/site-packages/sklearn/linear_model/_coordinate_descent.py:681: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 1.5901218428871289, tolerance: 0.5624595212816587\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/Users/anastasiafloris/miniconda3/envs/pKaPredict/lib/python3.13/site-packages/sklearn/linear_model/_coordinate_descent.py:681: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 2.891476213586202, tolerance: 0.5624595212816587\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/Users/anastasiafloris/miniconda3/envs/pKaPredict/lib/python3.13/site-packages/sklearn/linear_model/_coordinate_descent.py:681: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 2.341171076532646, tolerance: 0.5624595212816587\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/Users/anastasiafloris/miniconda3/envs/pKaPredict/lib/python3.13/site-packages/sklearn/linear_model/_coordinate_descent.py:681: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 1.6250125480680708, tolerance: 0.5624595212816587\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/Users/anastasiafloris/miniconda3/envs/pKaPredict/lib/python3.13/site-packages/sklearn/linear_model/_coordinate_descent.py:681: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 2.928027488645739, tolerance: 0.5624595212816587\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/Users/anastasiafloris/miniconda3/envs/pKaPredict/lib/python3.13/site-packages/sklearn/linear_model/_coordinate_descent.py:681: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 10.36817956648838, tolerance: 0.5624595212816587\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/Users/anastasiafloris/miniconda3/envs/pKaPredict/lib/python3.13/site-packages/sklearn/linear_model/_coordinate_descent.py:681: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 15.95419364031568, tolerance: 0.5624595212816587\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/Users/anastasiafloris/miniconda3/envs/pKaPredict/lib/python3.13/site-packages/sklearn/linear_model/_coordinate_descent.py:681: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 16.167091893028555, tolerance: 0.5624595212816587\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/Users/anastasiafloris/miniconda3/envs/pKaPredict/lib/python3.13/site-packages/sklearn/linear_model/_coordinate_descent.py:681: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 11.826990910354198, tolerance: 0.5624595212816587\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/Users/anastasiafloris/miniconda3/envs/pKaPredict/lib/python3.13/site-packages/sklearn/linear_model/_coordinate_descent.py:681: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 14.522524010059897, tolerance: 0.5624595212816587\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/Users/anastasiafloris/miniconda3/envs/pKaPredict/lib/python3.13/site-packages/sklearn/linear_model/_coordinate_descent.py:681: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 19.13330412331743, tolerance: 0.5624595212816587\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/Users/anastasiafloris/miniconda3/envs/pKaPredict/lib/python3.13/site-packages/sklearn/linear_model/_coordinate_descent.py:681: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 22.017133102863227, tolerance: 0.5624595212816587\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      " 50%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 21/42 [01:28<01:44,  4.97s/it]/Users/anastasiafloris/miniconda3/envs/pKaPredict/lib/python3.13/site-packages/sklearn/linear_model/_least_angle.py:723: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 40 iterations, i.e. alpha=1.102e-01, with an active set of 36 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "  warnings.warn(\n",
      "/Users/anastasiafloris/miniconda3/envs/pKaPredict/lib/python3.13/site-packages/sklearn/linear_model/_least_angle.py:753: ConvergenceWarning: Early stopping the lars path, as the residues are small and the current value of alpha is no longer well controlled. 52 iterations, alpha=8.533e-02, previous alpha=8.488e-02, with an active set of 45 regressors.\n",
      "  warnings.warn(\n",
      "/Users/anastasiafloris/miniconda3/envs/pKaPredict/lib/python3.13/site-packages/sklearn/linear_model/_least_angle.py:723: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 28 iterations, i.e. alpha=1.889e-01, with an active set of 24 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "  warnings.warn(\n",
      "/Users/anastasiafloris/miniconda3/envs/pKaPredict/lib/python3.13/site-packages/sklearn/linear_model/_least_angle.py:753: ConvergenceWarning: Early stopping the lars path, as the residues are small and the current value of alpha is no longer well controlled. 92 iterations, alpha=4.984e-02, previous alpha=4.945e-02, with an active set of 69 regressors.\n",
      "  warnings.warn(\n",
      "/Users/anastasiafloris/miniconda3/envs/pKaPredict/lib/python3.13/site-packages/sklearn/linear_model/_least_angle.py:723: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 78 iterations, i.e. alpha=4.784e-02, with an active set of 66 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "  warnings.warn(\n",
      "/Users/anastasiafloris/miniconda3/envs/pKaPredict/lib/python3.13/site-packages/sklearn/linear_model/_least_angle.py:723: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 79 iterations, i.e. alpha=4.773e-02, with an active set of 67 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "  warnings.warn(\n",
      "/Users/anastasiafloris/miniconda3/envs/pKaPredict/lib/python3.13/site-packages/sklearn/linear_model/_least_angle.py:753: ConvergenceWarning: Early stopping the lars path, as the residues are small and the current value of alpha is no longer well controlled. 84 iterations, alpha=4.220e-02, previous alpha=4.162e-02, with an active set of 71 regressors.\n",
      "  warnings.warn(\n",
      "/Users/anastasiafloris/miniconda3/envs/pKaPredict/lib/python3.13/site-packages/sklearn/linear_model/_least_angle.py:753: ConvergenceWarning: Early stopping the lars path, as the residues are small and the current value of alpha is no longer well controlled. 81 iterations, alpha=4.860e-02, previous alpha=4.785e-02, with an active set of 68 regressors.\n",
      "  warnings.warn(\n",
      "/Users/anastasiafloris/miniconda3/envs/pKaPredict/lib/python3.13/site-packages/sklearn/linear_model/_least_angle.py:723: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 49 iterations, i.e. alpha=7.913e-02, with an active set of 43 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "  warnings.warn(\n",
      "/Users/anastasiafloris/miniconda3/envs/pKaPredict/lib/python3.13/site-packages/sklearn/linear_model/_least_angle.py:753: ConvergenceWarning: Early stopping the lars path, as the residues are small and the current value of alpha is no longer well controlled. 80 iterations, alpha=4.309e-02, previous alpha=4.154e-02, with an active set of 67 regressors.\n",
      "  warnings.warn(\n",
      "/Users/anastasiafloris/miniconda3/envs/pKaPredict/lib/python3.13/site-packages/sklearn/linear_model/_least_angle.py:753: ConvergenceWarning: Early stopping the lars path, as the residues are small and the current value of alpha is no longer well controlled. 73 iterations, alpha=6.136e-02, previous alpha=5.990e-02, with an active set of 56 regressors.\n",
      "  warnings.warn(\n",
      " 52%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè    | 22/42 [01:29<01:15,  3.76s/it]/Users/anastasiafloris/miniconda3/envs/pKaPredict/lib/python3.13/site-packages/sklearn/linear_model/_least_angle.py:753: ConvergenceWarning: Early stopping the lars path, as the residues are small and the current value of alpha is no longer well controlled. 73 iterations, alpha=6.136e-02, previous alpha=5.990e-02, with an active set of 56 regressors.\n",
      "  warnings.warn(\n",
      " 57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 24/42 [01:29<00:35,  1.98s/it]/Users/anastasiafloris/miniconda3/envs/pKaPredict/lib/python3.13/site-packages/sklearn/svm/_base.py:1249: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      " 60%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ    | 25/42 [01:31<00:32,  1.88s/it]/Users/anastasiafloris/miniconda3/envs/pKaPredict/lib/python3.13/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      " 98%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä| 41/42 [03:06<00:03,  3.74s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.021543 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 19271\n",
      "[LightGBM] [Info] Number of data points in the train set: 1167, number of used features: 176\n",
      "[LightGBM] [Info] Start training from score 6.913366\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/anastasiafloris/miniconda3/envs/pKaPredict/lib/python3.13/site-packages/sklearn/utils/validation.py:2739: UserWarning: X does not have valid feature names, but LGBMRegressor was fitted with feature names\n",
      "  warnings.warn(\n",
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 42/42 [03:09<00:00,  4.52s/it]\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1200x600 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "\n",
    "from pkapredict.plotting import plot_best_models\n",
    "\n",
    "top_models = plot_best_models(X_train_scaled, X_valid_scaled, y_train, y_valid)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üå≤ *Machine learning model ü•á : ExtraTreesRegressor*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 36 candidates, totalling 108 fits\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[10], line 22\u001b[0m\n\u001b[1;32m     13\u001b[0m grid_search \u001b[38;5;241m=\u001b[39m GridSearchCV(\n\u001b[1;32m     14\u001b[0m     ExtraTreesRegressor(random_state\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m42\u001b[39m),\n\u001b[1;32m     15\u001b[0m     param_grid\u001b[38;5;241m=\u001b[39mparams,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     18\u001b[0m     n_jobs\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m\n\u001b[1;32m     19\u001b[0m )\n\u001b[1;32m     21\u001b[0m \u001b[38;5;66;03m# Fit GridSearchCV to the training data\u001b[39;00m\n\u001b[0;32m---> 22\u001b[0m \u001b[43mgrid_search\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train_scaled\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     24\u001b[0m \u001b[38;5;66;03m# Print best hyperparameters\u001b[39;00m\n\u001b[1;32m     25\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mOptimized parameters for ExtraTreesRegressor:\u001b[39m\u001b[38;5;124m\"\u001b[39m, grid_search\u001b[38;5;241m.\u001b[39mbest_params_)\n",
      "File \u001b[0;32m~/miniconda3/envs/pKaPredict/lib/python3.13/site-packages/sklearn/base.py:1389\u001b[0m, in \u001b[0;36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[0;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1382\u001b[0m     estimator\u001b[38;5;241m.\u001b[39m_validate_params()\n\u001b[1;32m   1384\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[1;32m   1385\u001b[0m     skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[1;32m   1386\u001b[0m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[1;32m   1387\u001b[0m     )\n\u001b[1;32m   1388\u001b[0m ):\n\u001b[0;32m-> 1389\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfit_method\u001b[49m\u001b[43m(\u001b[49m\u001b[43mestimator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/pKaPredict/lib/python3.13/site-packages/sklearn/model_selection/_search.py:1024\u001b[0m, in \u001b[0;36mBaseSearchCV.fit\u001b[0;34m(self, X, y, **params)\u001b[0m\n\u001b[1;32m   1018\u001b[0m     results \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_format_results(\n\u001b[1;32m   1019\u001b[0m         all_candidate_params, n_splits, all_out, all_more_results\n\u001b[1;32m   1020\u001b[0m     )\n\u001b[1;32m   1022\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m results\n\u001b[0;32m-> 1024\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_run_search\u001b[49m\u001b[43m(\u001b[49m\u001b[43mevaluate_candidates\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1026\u001b[0m \u001b[38;5;66;03m# multimetric is determined here because in the case of a callable\u001b[39;00m\n\u001b[1;32m   1027\u001b[0m \u001b[38;5;66;03m# self.scoring the return type is only known after calling\u001b[39;00m\n\u001b[1;32m   1028\u001b[0m first_test_score \u001b[38;5;241m=\u001b[39m all_out[\u001b[38;5;241m0\u001b[39m][\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtest_scores\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n",
      "File \u001b[0;32m~/miniconda3/envs/pKaPredict/lib/python3.13/site-packages/sklearn/model_selection/_search.py:1571\u001b[0m, in \u001b[0;36mGridSearchCV._run_search\u001b[0;34m(self, evaluate_candidates)\u001b[0m\n\u001b[1;32m   1569\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m_run_search\u001b[39m(\u001b[38;5;28mself\u001b[39m, evaluate_candidates):\n\u001b[1;32m   1570\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Search all candidates in param_grid\"\"\"\u001b[39;00m\n\u001b[0;32m-> 1571\u001b[0m     \u001b[43mevaluate_candidates\u001b[49m\u001b[43m(\u001b[49m\u001b[43mParameterGrid\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mparam_grid\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/pKaPredict/lib/python3.13/site-packages/sklearn/model_selection/_search.py:970\u001b[0m, in \u001b[0;36mBaseSearchCV.fit.<locals>.evaluate_candidates\u001b[0;34m(candidate_params, cv, more_results)\u001b[0m\n\u001b[1;32m    962\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mverbose \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m    963\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\n\u001b[1;32m    964\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFitting \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;124m folds for each of \u001b[39m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;124m candidates,\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    965\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m totalling \u001b[39m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m fits\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\n\u001b[1;32m    966\u001b[0m             n_splits, n_candidates, n_candidates \u001b[38;5;241m*\u001b[39m n_splits\n\u001b[1;32m    967\u001b[0m         )\n\u001b[1;32m    968\u001b[0m     )\n\u001b[0;32m--> 970\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[43mparallel\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    971\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdelayed\u001b[49m\u001b[43m(\u001b[49m\u001b[43m_fit_and_score\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    972\u001b[0m \u001b[43m        \u001b[49m\u001b[43mclone\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbase_estimator\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    973\u001b[0m \u001b[43m        \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    974\u001b[0m \u001b[43m        \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    975\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtrain\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    976\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtest\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtest\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    977\u001b[0m \u001b[43m        \u001b[49m\u001b[43mparameters\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mparameters\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    978\u001b[0m \u001b[43m        \u001b[49m\u001b[43msplit_progress\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43msplit_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_splits\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    979\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcandidate_progress\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mcand_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_candidates\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    980\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mfit_and_score_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    981\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    982\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mcand_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparameters\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43msplit_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mproduct\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    983\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43menumerate\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mcandidate_params\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    984\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43menumerate\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mcv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msplit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mrouted_params\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msplitter\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msplit\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    985\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    986\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    988\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(out) \u001b[38;5;241m<\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m    989\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    990\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNo fits were performed. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    991\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWas the CV iterator empty? \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    992\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWere there no candidates?\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    993\u001b[0m     )\n",
      "File \u001b[0;32m~/miniconda3/envs/pKaPredict/lib/python3.13/site-packages/sklearn/utils/parallel.py:77\u001b[0m, in \u001b[0;36mParallel.__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m     72\u001b[0m config \u001b[38;5;241m=\u001b[39m get_config()\n\u001b[1;32m     73\u001b[0m iterable_with_config \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m     74\u001b[0m     (_with_config(delayed_func, config), args, kwargs)\n\u001b[1;32m     75\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m delayed_func, args, kwargs \u001b[38;5;129;01min\u001b[39;00m iterable\n\u001b[1;32m     76\u001b[0m )\n\u001b[0;32m---> 77\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__call__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43miterable_with_config\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/pKaPredict/lib/python3.13/site-packages/joblib/parallel.py:2007\u001b[0m, in \u001b[0;36mParallel.__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m   2001\u001b[0m \u001b[38;5;66;03m# The first item from the output is blank, but it makes the interpreter\u001b[39;00m\n\u001b[1;32m   2002\u001b[0m \u001b[38;5;66;03m# progress until it enters the Try/Except block of the generator and\u001b[39;00m\n\u001b[1;32m   2003\u001b[0m \u001b[38;5;66;03m# reaches the first `yield` statement. This starts the asynchronous\u001b[39;00m\n\u001b[1;32m   2004\u001b[0m \u001b[38;5;66;03m# dispatch of the tasks to the workers.\u001b[39;00m\n\u001b[1;32m   2005\u001b[0m \u001b[38;5;28mnext\u001b[39m(output)\n\u001b[0;32m-> 2007\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m output \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreturn_generator \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43moutput\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/pKaPredict/lib/python3.13/site-packages/joblib/parallel.py:1650\u001b[0m, in \u001b[0;36mParallel._get_outputs\u001b[0;34m(self, iterator, pre_dispatch)\u001b[0m\n\u001b[1;32m   1647\u001b[0m     \u001b[38;5;28;01myield\u001b[39;00m\n\u001b[1;32m   1649\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backend\u001b[38;5;241m.\u001b[39mretrieval_context():\n\u001b[0;32m-> 1650\u001b[0m         \u001b[38;5;28;01myield from\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_retrieve()\n\u001b[1;32m   1652\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mGeneratorExit\u001b[39;00m:\n\u001b[1;32m   1653\u001b[0m     \u001b[38;5;66;03m# The generator has been garbage collected before being fully\u001b[39;00m\n\u001b[1;32m   1654\u001b[0m     \u001b[38;5;66;03m# consumed. This aborts the remaining tasks if possible and warn\u001b[39;00m\n\u001b[1;32m   1655\u001b[0m     \u001b[38;5;66;03m# the user if necessary.\u001b[39;00m\n\u001b[1;32m   1656\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_exception \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/pKaPredict/lib/python3.13/site-packages/joblib/parallel.py:1762\u001b[0m, in \u001b[0;36mParallel._retrieve\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1757\u001b[0m \u001b[38;5;66;03m# If the next job is not ready for retrieval yet, we just wait for\u001b[39;00m\n\u001b[1;32m   1758\u001b[0m \u001b[38;5;66;03m# async callbacks to progress.\u001b[39;00m\n\u001b[1;32m   1759\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m ((\u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jobs) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m\n\u001b[1;32m   1760\u001b[0m     (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jobs[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mget_status(\n\u001b[1;32m   1761\u001b[0m         timeout\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtimeout) \u001b[38;5;241m==\u001b[39m TASK_PENDING)):\n\u001b[0;32m-> 1762\u001b[0m     \u001b[43mtime\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msleep\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m0.01\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1763\u001b[0m     \u001b[38;5;28;01mcontinue\u001b[39;00m\n\u001b[1;32m   1765\u001b[0m \u001b[38;5;66;03m# We need to be careful: the job list can be filling up as\u001b[39;00m\n\u001b[1;32m   1766\u001b[0m \u001b[38;5;66;03m# we empty it and Python list are not thread-safe by\u001b[39;00m\n\u001b[1;32m   1767\u001b[0m \u001b[38;5;66;03m# default hence the use of the lock\u001b[39;00m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# =============================\n",
    "# 1st Grid Search for ExtraTreesRegressor\n",
    "# =============================\n",
    "\n",
    "# Define initial hyperparameter grid\n",
    "params = {\n",
    "    \"max_depth\": list(range(15, 26, 5)),  # Trying depths between 10 and 30\n",
    "    \"n_estimators\": list(range(100, 500, 100)),  # Number of trees between 100 and 500\n",
    "    \"min_samples_split\": [2, 5, 10],  # Minimum samples to split a node\n",
    "}\n",
    "\n",
    "# Initialize GridSearchCV\n",
    "grid_search = GridSearchCV(\n",
    "    ExtraTreesRegressor(random_state=42),\n",
    "    param_grid=params,\n",
    "    cv=3,\n",
    "    verbose=1,\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "# Fit GridSearchCV to the training data\n",
    "grid_search.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Print best hyperparameters\n",
    "print(\"Optimized parameters for ExtraTreesRegressor:\", grid_search.best_params_)\n",
    "\n",
    "# =============================\n",
    "# 2nd Grid Search for Further Optimization\n",
    "# =============================\n",
    "\n",
    "# Extract best hyperparameters from first search\n",
    "best_params = grid_search.best_params_\n",
    "\n",
    "# Define refined hyperparameter grid\n",
    "params_bst = {\n",
    "    \"max_depth\": list(range(best_params[\"max_depth\"] - 2, best_params[\"max_depth\"] + 3, 1)),\n",
    "    \"n_estimators\": list(range(best_params[\"n_estimators\"] - 50, best_params[\"n_estimators\"] + 100, 50)),\n",
    "    \"min_samples_split\": [best_params[\"min_samples_split\"] - 1, best_params[\"min_samples_split\"], best_params[\"min_samples_split\"] + 1],\n",
    "}\n",
    "\n",
    "# Initialize refined GridSearchCV\n",
    "grid_search_bst = GridSearchCV(\n",
    "    ExtraTreesRegressor(random_state=42),\n",
    "    param_grid=params_bst,\n",
    "    cv=3,\n",
    "    verbose=1,\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "# Fit GridSearchCV to the training data\n",
    "grid_search_bst.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Print the best hyperparameters\n",
    "print(\"The best parameters after further optimization:\", grid_search_bst.best_params_)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The hyperparameter tuning for ExtraTreesRegressor was conducted in two stages to identify the optimal parameters for predicting pKa values efficiently. In the first grid search, 36 different hyperparameter combinations were tested using 3-fold cross-validation (cv=3) instead of 5-fold to significantly reduce computational time while maintaining accuracy. This resulted in an initial best model with max_depth = 25, min_samples_split = 2, and n_estimators = 100. To further refine the model, a second grid search was conducted, focusing on a narrower range of max_depth (15 to 26 instead of 10 to 31) and adjusting n_estimators, leading to the final optimized values: max_depth = 23, min_samples_split = 2, and n_estimators = 150. These refinements balanced model complexity and efficiency, ensuring improved generalization while significantly reducing training time. üöÄ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# =============================\n",
    "# Train ExtraTreesRegressor with Optimized Parameters\n",
    "# =============================\n",
    "\n",
    "# Define the best parameters found from GridSearchCV\n",
    "best_params = {'max_depth': 23, 'min_samples_split': 2, 'n_estimators': 150}\n",
    "\n",
    "# Initialize the model with optimized parameters\n",
    "model = ExtraTreesRegressor(\n",
    "    n_estimators=best_params[\"n_estimators\"],\n",
    "    max_depth=best_params[\"max_depth\"],\n",
    "    min_samples_split=best_params[\"min_samples_split\"],\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "# Train the model\n",
    "model.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Make predictions on the validation set\n",
    "y_preds = model.predict(X_valid_scaled)\n",
    "\n",
    "# =============================\n",
    "# Define Function for Plotting\n",
    "# =============================\n",
    "\n",
    "from pkapredict.plot_data import plot_data\n",
    "\n",
    "# =============================\n",
    "# Plot and Save Results\n",
    "# =============================\n",
    "\n",
    "# Plot validation results\n",
    "plot_data(y_valid, y_preds, \"Validation Data - ExtraTreesRegressor\")\n",
    "\n",
    "# Define save path in the specified folder\n",
    "save_path = os.path.join(\"..\", \"notebooks\", \"Plots\", \"validation_ExtraTrees.png\")\n",
    "\n",
    "# Save figure as PNG in the correct directory\n",
    "plt.savefig(save_path, dpi=300, bbox_inches='tight')\n",
    "\n",
    "# Success message\n",
    "print(\"Plot saved as validation_ExtraTrees.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "üì∂ Optimization of a supplementary parameter of this model through the selection of the most relavent descriptors to pKa prediction using SelectKBest. The following code plots all R¬≤ values as a function of k values and prints the optimal one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Define range of k values (from 1 to total number of features, step 1)\n",
    "k_values = list(range(1, X_train_scaled.shape[1] + 1))\n",
    "\n",
    "# Store R¬≤ scores for each k\n",
    "r2_scores = []\n",
    "\n",
    "# Loop through k values and train ExtraTreesRegressor\n",
    "for k in k_values:\n",
    "    # Select top k features\n",
    "    selector = SelectKBest(score_func=f_regression, k=k)\n",
    "    X_train_selected = selector.fit_transform(X_train_scaled, y_train)\n",
    "    X_valid_selected = selector.transform(X_valid_scaled)\n",
    "\n",
    "    # Train ExtraTreesRegressor with selected features\n",
    "    model = ExtraTreesRegressor(n_estimators=150, max_depth=23, min_samples_split=2, random_state=42)\n",
    "    model.fit(X_train_selected, y_train)\n",
    "\n",
    "    # Make predictions\n",
    "    y_preds = model.predict(X_valid_selected)\n",
    "\n",
    "    # Compute R¬≤ score\n",
    "    r2 = r2_score(y_valid, y_preds)\n",
    "    r2_scores.append(r2)\n",
    "\n",
    "# Find the optimal k value\n",
    "optimal_k = k_values[np.argmax(r2_scores)]\n",
    "best_r2 = max(r2_scores)\n",
    "\n",
    "# Print optimal k value with highest R¬≤\n",
    "print(f\"Optimal k: {optimal_k} with Highest R¬≤: {best_r2:.4f}\")\n",
    "\n",
    "from pKaPredict.visualization import plot_k_vs_r2_ET\n",
    "\n",
    "# Now use the function\n",
    "path = plot_k_vs_r2_ET(k_values, r2_scores)\n",
    "print(\"Plot saved to:\", path)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "üí™üèª Plot of the data with the optimized machine leaning parameters of this model!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# =============================\n",
    "# Feature Selection with SelectKBest (k=95)\n",
    "# =============================\n",
    "\n",
    "k = 95  # Number of best features to select\n",
    "selector = SelectKBest(score_func=f_regression, k=k)\n",
    "X_train_selected = selector.fit_transform(X_train_scaled, y_train)\n",
    "X_valid_selected = selector.transform(X_valid_scaled)\n",
    "\n",
    "# =============================\n",
    "# Train ExtraTreesRegressor with Optimized Parameters\n",
    "# =============================\n",
    "\n",
    "# Define the best parameters found from GridSearchCV\n",
    "best_params = {'max_depth': 23, 'min_samples_split': 2, 'n_estimators': 150}\n",
    "\n",
    "# Initialize the model with optimized parameters\n",
    "model = ExtraTreesRegressor(\n",
    "    n_estimators=best_params[\"n_estimators\"],\n",
    "    max_depth=best_params[\"max_depth\"],\n",
    "    min_samples_split=best_params[\"min_samples_split\"],\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "# Train the model on the selected features\n",
    "model.fit(X_train_selected, y_train)\n",
    "\n",
    "# Make predictions on the validation set\n",
    "y_preds = model.predict(X_valid_selected)\n",
    "\n",
    "# =============================\n",
    "# Define Function for Plotting\n",
    "# =============================\n",
    "\n",
    "from pkapredict.plot_data import plot_data\n",
    "\n",
    "# =============================\n",
    "# Plot and Save Results\n",
    "# =============================\n",
    "\n",
    "# Plot validation results\n",
    "plot_data(y_valid, y_preds, \"Validation Optimized Data - ExtraTreesRegressor\")\n",
    "\n",
    "# Define save path in the specified folder\n",
    "save_path = os.path.join(\"..\", \"notebooks\", \"Plots\", \"validationoptimized_ExtraTrees.png\")\n",
    "\n",
    "# Save figure as PNG in the correct directory\n",
    "plt.savefig(save_path, dpi=300, bbox_inches='tight')\n",
    "\n",
    "# Success message\n",
    "print(\"Plot saved as validationoptimized_ExtraTrees.png\")\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "‚ö°Ô∏è Computation of statistics for this model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# =============================\n",
    "# Compute Model Performance Metrics\n",
    "# =============================\n",
    "\n",
    "# Compute mean error\n",
    "mean_error = np.mean(y_preds - y_valid)\n",
    "\n",
    "# Compute variance of the error\n",
    "variance_error = np.var(y_preds - y_valid)\n",
    "\n",
    "# Compute mean predicted value\n",
    "mean_predicted = np.mean(y_preds)\n",
    "\n",
    "# Compute Mean Squared Error (MSE)\n",
    "mse = mean_squared_error(y_valid, y_preds)\n",
    "\n",
    "# Compute R¬≤ score\n",
    "r2 = r2_score(y_valid, y_preds)\n",
    "\n",
    "# Compute 95% confidence interval for predictions\n",
    "confidence = 0.95\n",
    "n = len(y_preds)\n",
    "std_err = stats.sem(y_preds - y_valid)\n",
    "margin_of_error = std_err * stats.t.ppf((1 + confidence) / 2., n-1)\n",
    "confidence_interval = (mean_error - margin_of_error, mean_error + margin_of_error)\n",
    "\n",
    "# Print computed metrics\n",
    "print(f\"Mean Error: {mean_error:.4f}\")\n",
    "print(f\"Variance of Error: {variance_error:.4f}\")\n",
    "print(f\"Mean Predicted Value: {mean_predicted:.4f}\")\n",
    "print(f\"MSE: {mse:.4f}\")\n",
    "print(f\"R¬≤: {r2:.4f}\")\n",
    "print(f\"95% Confidence Interval: {confidence_interval}\")\n",
    "\n",
    "# Create a DataFrame for storing the results\n",
    "stats_df = pd.DataFrame({\n",
    "    \"Metric\": [\"Mean Error\", \"Variance\", \"Mean Predicted Value\", \"MSE\", \"R¬≤\", \"95% CI Lower\", \"95% CI Upper\"],\n",
    "    \"Value\": [mean_error, variance_error, mean_predicted, mse, r2, confidence_interval[0], confidence_interval[1]]\n",
    "})\n",
    "\n",
    "\n",
    "stats_save_path = os.path.join(\"..\", \"data\", \"ExtraTreesRegressor_stats.csv\")\n",
    "stats_df.to_csv(stats_save_path, index=False)\n",
    "print(f\"Table saved at: {stats_save_path}\")\n",
    "\n",
    "# Display as an interactive HTML table\n",
    "ipd.display(ipd.HTML(stats_df.to_html(classes='table table-striped', escape=False)))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ü§ñ *Machine learning model ü•à : LGBMRegressor*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# =============================\n",
    "# Load and Prepare Data\n",
    "# =============================\n",
    "\n",
    "# Define file paths\n",
    "file_path1 = os.path.join(\"..\", \"data\", \"Data_pKa_Descriptors.csv\")  # Descriptors (X)\n",
    "file_path2 = os.path.join(\"..\", \"data\", \"pKadata_cleaned.csv\")  # pKa values (y)\n",
    "\n",
    "# Load the datasets\n",
    "df_descriptors = pd.read_csv(file_path1)  # Features (X)\n",
    "df_pKa = pd.read_csv(file_path2)  # Target values (y)\n",
    "\n",
    "# Merge descriptors with pKa values using 'Smiles'\n",
    "df_merged = df_descriptors.merge(df_pKa[['pka', 'Smiles']], on='Smiles')\n",
    "\n",
    "# Drop non-numeric columns (Smiles)\n",
    "X = df_merged.drop(columns=['pka', 'Smiles'])  # Drop pKa (target) and Smiles (string)\n",
    "y = df_merged['pka']  # Target variable (pKa values)\n",
    "\n",
    "# Split data into training (90%) and validation (10%) sets\n",
    "X_train, X_valid, y_train, y_valid = train_test_split(X, y, test_size=0.1, random_state=42)\n",
    "\n",
    "# Standardize the features\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_valid_scaled = scaler.transform(X_valid)\n",
    "\n",
    "# Convert back to DataFrame to avoid warnings in prediction\n",
    "X_train_scaled_df = pd.DataFrame(X_train_scaled, columns=X.columns)\n",
    "X_valid_scaled_df = pd.DataFrame(X_valid_scaled, columns=X.columns)\n",
    "\n",
    "# =============================\n",
    "# 1st Grid Search for LGBMRegressor (Optimized for Speed)\n",
    "# =============================\n",
    "\n",
    "# Start timing\n",
    "start_time = time.time()\n",
    "\n",
    "# Define initial hyperparameter grid with reduced values for speed\n",
    "params = {\n",
    "    \"max_depth\": [10, 15],  # Smaller range\n",
    "    \"n_estimators\": [100, 200],  # Fewer estimators\n",
    "    \"learning_rate\": [0.05, 0.1],  # Limited choices\n",
    "    \"num_leaves\": [20, 31],  # Small variation\n",
    "    \"min_child_samples\": [10, 20]  # Small variation\n",
    "}\n",
    "\n",
    "# Initialize GridSearchCV\n",
    "grid_search = GridSearchCV(\n",
    "    LGBMRegressor(random_state=42, verbose=-1),  \n",
    "    param_grid=params,\n",
    "    cv=2,  \n",
    "    verbose=0,  \n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "# Fit GridSearchCV to the training data\n",
    "grid_search.fit(X_train_scaled_df, y_train)\n",
    "\n",
    "# Print best hyperparameters\n",
    "print(\"Optimized parameters for LGBMRegressor:\", grid_search.best_params_)\n",
    "\n",
    "# =============================\n",
    "# 2nd Grid Search for Further Optimization\n",
    "# =============================\n",
    "\n",
    "# Extract best hyperparameters from first search\n",
    "best_params = grid_search.best_params_\n",
    "\n",
    "# Define refined hyperparameter grid\n",
    "params_bst = {\n",
    "    \"max_depth\": [best_params[\"max_depth\"] - 1, best_params[\"max_depth\"], best_params[\"max_depth\"] + 1],\n",
    "    \"n_estimators\": [best_params[\"n_estimators\"] - 50, best_params[\"n_estimators\"] + 50],\n",
    "    \"learning_rate\": [best_params[\"learning_rate\"] * 0.9, best_params[\"learning_rate\"], best_params[\"learning_rate\"] * 1.1],\n",
    "    \"num_leaves\": [best_params[\"num_leaves\"] - 5, best_params[\"num_leaves\"] + 5],\n",
    "    \"min_child_samples\": [best_params[\"min_child_samples\"] - 5, best_params[\"min_child_samples\"] + 5]\n",
    "}\n",
    "\n",
    "# Initialize refined GridSearchCV\n",
    "grid_search_bst = GridSearchCV(\n",
    "    LGBMRegressor(random_state=42, verbose=-1),  \n",
    "    param_grid=params_bst,\n",
    "    cv=2,  \n",
    "    verbose=0, \n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "# Fit GridSearchCV to the training data\n",
    "grid_search_bst.fit(X_train_scaled_df, y_train)\n",
    "\n",
    "# Print the best hyperparameters\n",
    "print(\"The best parameters after further optimization:\", grid_search_bst.best_params_)\n",
    "\n",
    "# End timing\n",
    "end_time = time.time()\n",
    "print(f\"Total execution time: {end_time - start_time:.2f} seconds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ü•≥ Implementation of the optimized parameters and plotting of the data distribution as well as the associated R¬≤ value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# =============================\n",
    "# Train LGBMRegressor with Optimized Parameters\n",
    "# =============================\n",
    "\n",
    "# Define the best parameters found from GridSearchCV\n",
    "best_params = {\n",
    "    'learning_rate': 0.045000000000000005,\n",
    "    'max_depth': 9,\n",
    "    'min_child_samples': 5,\n",
    "    'n_estimators': 250,\n",
    "    'num_leaves': 15\n",
    "}\n",
    "\n",
    "# Initialize the model with optimized parameters\n",
    "model2 = LGBMRegressor(\n",
    "    learning_rate=best_params['learning_rate'],\n",
    "    max_depth=best_params['max_depth'],\n",
    "    min_child_samples=best_params['min_child_samples'],\n",
    "    n_estimators=best_params['n_estimators'],\n",
    "    num_leaves=best_params['num_leaves'],\n",
    "    random_state=42,\n",
    "    verbose=-1  \n",
    ")\n",
    "\n",
    "# Train the model\n",
    "model2.fit(X_train_scaled_df, y_train)\n",
    "\n",
    "# Make predictions on the validation set\n",
    "y_preds = model2.predict(X_valid_scaled_df)\n",
    "\n",
    "# =============================\n",
    "# Define Function for Plotting\n",
    "# =============================\n",
    "\n",
    "\n",
    "\n",
    "# Save figure\n",
    "plt.savefig(save_path, dpi=300, bbox_inches='tight')\n",
    "    \n",
    "\n",
    "\n",
    "# =============================\n",
    "# Plot and Save Results\n",
    "# =============================\n",
    "\n",
    "# Define save path in the specified folder\n",
    "save_path = os.path.join(\"..\", \"notebooks\", \"Plots\", \"validation_LGBMRegressor.png\")\n",
    "\n",
    "# Plot validation results and display the plot\n",
    "plot_data(y_valid, y_preds, \"Validation Data - LGBMRegressor\")\n",
    "\n",
    "# Success message\n",
    "print(\"Plot saved as validation_LGBMRegressor.png\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "üì∂ Optimization of a supplementary parameter of this model by selecting the most relavent descriptors to pKa prediction using SelectKBest. The following code plots all R¬≤ values as a function of k values and prints the optimal one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Define range of k values (from 1 to total number of features, step 1)\n",
    "k_values = list(range(1, X_train_scaled_df.shape[1] + 1))\n",
    "\n",
    "# Store R¬≤ scores for each k\n",
    "r2_scores = []\n",
    "\n",
    "# Loop through k values and train LGBMRegressor\n",
    "for k in k_values:\n",
    "    # Select top k features\n",
    "    selector = SelectKBest(score_func=f_regression, k=k)\n",
    "    X_train_selected = selector.fit_transform(X_train_scaled_df, y_train)\n",
    "    X_valid_selected = selector.transform(X_valid_scaled_df)\n",
    "\n",
    "    # Convert selected features back to DataFrame to retain feature names\n",
    "    selected_features = X_train_scaled_df.columns[selector.get_support()]\n",
    "    X_train_selected_df = pd.DataFrame(X_train_selected, columns=selected_features)\n",
    "    X_valid_selected_df = pd.DataFrame(X_valid_selected, columns=selected_features)\n",
    "\n",
    "    # Train LGBMRegressor with selected features\n",
    "    model2 = LGBMRegressor(\n",
    "        learning_rate=0.045,\n",
    "        max_depth=9,\n",
    "        min_child_samples=5,\n",
    "        n_estimators=250,\n",
    "        num_leaves=15,\n",
    "        random_state=42,\n",
    "        verbose=-1\n",
    "    )\n",
    "    model2.fit(X_train_selected_df, y_train)\n",
    "\n",
    "    # Make predictions\n",
    "    y_preds = model2.predict(X_valid_selected_df)\n",
    "\n",
    "    # Compute R¬≤ score\n",
    "    r2 = r2_score(y_valid, y_preds)\n",
    "    r2_scores.append(r2)\n",
    "\n",
    "# Find the optimal k value\n",
    "optimal_k = k_values[np.argmax(r2_scores)]\n",
    "best_r2 = max(r2_scores)\n",
    "\n",
    "# Print optimal k value with highest R¬≤\n",
    "print(f\"Optimal k: {optimal_k} with Highest R¬≤: {best_r2:.4f}\")\n",
    "\n",
    "# Plot R¬≤ Scores vs. k Values\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(k_values, r2_scores, marker='o', linestyle='-', color='lightpink')\n",
    "plt.xlabel(\"Number of Features (k)\", color=\"black\")\n",
    "plt.ylabel(\"R¬≤ Score\", color=\"black\")\n",
    "plt.title(\"Optimal k for SelectKBest Feature Selection\", color=\"black\")\n",
    "plt.gca().set_facecolor('white')  \n",
    "plt.tick_params(axis='both', colors='black')  \n",
    "\n",
    "# Define save path in the specified folder\n",
    "save_path = os.path.join(\"..\", \"notebooks\", \"Plots\", \"optimalkvalue.png\")\n",
    "\n",
    "# Save figure as PNG in the correct directory\n",
    "plt.savefig(save_path, dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "# Success message\n",
    "print(\"Plot saved as optimalkvalue.png\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "üí™üèª Plotting of the data with the optimized machine leaning parameters of this model!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# =============================\n",
    "# Feature Selection with SelectKBest (k=142)\n",
    "# =============================\n",
    "\n",
    "k = 142  # Optimized number of best features for LGBMRegressor\n",
    "selector = SelectKBest(score_func=f_regression, k=k)\n",
    "X_train_selected = selector.fit_transform(X_train_scaled_df, y_train)\n",
    "X_valid_selected = selector.transform(X_valid_scaled_df)\n",
    "\n",
    "# Convert selected features back to DataFrame to retain feature names\n",
    "selected_features = X_train_scaled_df.columns[selector.get_support()]\n",
    "X_train_selected_df = pd.DataFrame(X_train_selected, columns=selected_features)\n",
    "X_valid_selected_df = pd.DataFrame(X_valid_selected, columns=selected_features)\n",
    "\n",
    "# =============================\n",
    "# Train LGBMRegressor with Optimized Parameters\n",
    "# =============================\n",
    "\n",
    "# Define the best parameters found from GridSearchCV\n",
    "best_params = {\n",
    "    'learning_rate': 0.045,\n",
    "    'max_depth': 9,\n",
    "    'min_child_samples': 5,\n",
    "    'n_estimators': 250,\n",
    "    'num_leaves': 15\n",
    "}\n",
    "\n",
    "# Initialize the model with optimized parameters\n",
    "model2 = LGBMRegressor(\n",
    "    learning_rate=best_params['learning_rate'],\n",
    "    max_depth=best_params['max_depth'],\n",
    "    min_child_samples=best_params['min_child_samples'],\n",
    "    n_estimators=best_params['n_estimators'],\n",
    "    num_leaves=best_params['num_leaves'],\n",
    "    random_state=42,\n",
    "    verbose=-1\n",
    ")\n",
    "\n",
    "# Train the model on the selected features\n",
    "model2.fit(X_train_selected_df, y_train)\n",
    "\n",
    "# Make predictions on the validation set\n",
    "y_preds = model2.predict(X_valid_selected_df)\n",
    "\n",
    "# =============================\n",
    "# Define Function for Plotting\n",
    "# =============================\n",
    "\n",
    "\n",
    "\n",
    "# Save figure\n",
    "plt.savefig(save_path, dpi=300, bbox_inches='tight')\n",
    "    \n",
    "\n",
    "\n",
    "# =============================\n",
    "# Plot and Save Results\n",
    "# =============================\n",
    "\n",
    "# Define save path in the specified folder\n",
    "save_path = os.path.join(\"..\", \"notebooks\", \"Plots\", \"validationoptimized_LGBMRegressor.png\")\n",
    "\n",
    "# Plot validation results and display the plot\n",
    "plot_data(y_valid, y_preds, \"Validation Optimized Data - LGBMRegressor\")\n",
    "\n",
    "# Success message\n",
    "print(\"Plot saved as validationoptimized_LGBMRegressor.png\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "‚ö°Ô∏è Computation of statistics for this model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Compute mean error\n",
    "mean_error2 = np.mean(y_preds - y_valid)\n",
    "\n",
    "# Compute variance of the error\n",
    "variance_error2 = np.var(y_preds - y_valid)\n",
    "\n",
    "# Compute mean predicted value\n",
    "mean_predicted2 = np.mean(y_preds)\n",
    "\n",
    "# Compute Mean Squared Error (MSE)\n",
    "mse2 = mean_squared_error(y_valid, y_preds)\n",
    "\n",
    "# Compute R¬≤ score\n",
    "r2_2 = r2_score(y_valid, y_preds)\n",
    "\n",
    "# Compute 95% confidence interval for predictions\n",
    "confidence2 = 0.95\n",
    "n = len(y_preds)\n",
    "std_err2 = stats.sem(y_preds - y_valid)\n",
    "margin_of_error2 = std_err2 * stats.t.ppf((1 + confidence2) / 2., n-1)\n",
    "confidence_interval2 = (mean_error2 - margin_of_error2, mean_error2 + margin_of_error2)\n",
    "\n",
    "# Print computed metrics\n",
    "print(f\"Mean Error: {mean_error2:.4f}\")\n",
    "print(f\"Variance of Error: {variance_error2:.4f}\")\n",
    "print(f\"Mean Predicted Value: {mean_predicted2:.4f}\")\n",
    "print(f\"MSE: {mse2:.4f}\")\n",
    "print(f\"R¬≤: {r2_2:.4f}\")\n",
    "print(f\"95% Confidence Interval: {confidence_interval2}\")\n",
    "\n",
    "# Create a DataFrame for storing the results\n",
    "stats_df = pd.DataFrame({\n",
    "    \"Metric\": [\"Mean Error\", \"Variance\", \"Mean Predicted Value\", \"MSE\", \"R¬≤\", \"95% CI Lower\", \"95% CI Upper\"],\n",
    "    \"Value\": [mean_error2, variance_error2, mean_predicted2, mse2, r2_2, confidence_interval2[0], confidence_interval2[1]]\n",
    "})\n",
    "\n",
    "\n",
    "save_path = os.path.join(\"..\", \"data\",\"LGBM_regressor_stats.csv\")\n",
    "stats_df.to_csv(save_path, index=False)\n",
    "print(f\"Table saved at: {save_path}\")\n",
    "\n",
    "# Display as an interactive HTML table\n",
    "ipd.display(ipd.HTML(stats_df.to_html(classes='table table-striped', escape=False)))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üßê *Comparison of the two machine learning models*\n",
    "\n",
    "When evaluating the machine learning models for pKa prediction, R¬≤ (coefficient of determination) and RMSE (Root Mean Squared Error) are two key metrics used to assess model performance. R¬≤ measures how well the model explains the variance in the experimental pKa values, ranging from 0 (no predictive power) to 1 (perfect prediction). RMSE quantifies the average difference between predicted and actual values, with lower RMSE indicating more accurate predictions.\n",
    "\n",
    "Initially, ExtraTreesRegressor and LGBMRegressor emerged as the top performers based on their R¬≤ scores. However, the final figures demonstrate that both models benefited from hyperparameter tuning and the optimization of k, the number of most relevant descriptors selected using SelectKBest. After optimization, LGBMRegressor achieved the best performance with an R¬≤ of 0.68 and an RMSE of 1.37, while ExtraTreesRegressor followed closely with an R¬≤ of 0.67 and an RMSE of 1.39. The scatter plots confirm strong correlations between predicted and experimental pKa values, with the optimized models producing more accurate and reliable predictions. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Define file paths\n",
    "path1 = os.path.join(\"..\", \"data\", \"ExtraTreesRegressor_stats.csv\")\n",
    "path2 = os.path.join(\"..\", \"data\", \"LGBM_regressor_stats.csv\")\n",
    "\n",
    "# Load data\n",
    "extra_trees_df = pd.read_csv(path1)\n",
    "lgbm_df = pd.read_csv(path2)\n",
    "\n",
    "# Merge data for comparison\n",
    "comparison_df = extra_trees_df.merge(lgbm_df, on=\"Metric\", suffixes=(\"_ExtraTrees\", \"_LGBM\"))\n",
    "\n",
    "# Define metrics and values\n",
    "metrics = comparison_df[\"Metric\"]\n",
    "values_extra_trees = comparison_df[\"Value_ExtraTrees\"]\n",
    "values_lgbm = comparison_df[\"Value_LGBM\"]\n",
    "\n",
    "# Set up subplot grid (4 columns, 3 rows)\n",
    "n_cols = 4\n",
    "n_rows = int(np.ceil(len(metrics) / n_cols))\n",
    "fig, axes = plt.subplots(n_rows, n_cols, figsize=(n_cols * 4, n_rows * 3), sharey=False, facecolor='white')\n",
    "axes = axes.flatten()\n",
    "\n",
    "for i, metric in enumerate(metrics):\n",
    "    ax = axes[i]\n",
    "    x_center = 0.5  \n",
    "    colors = ['#A6D785', '#FF69B4'] \n",
    "    labels = [\"ExtraTrees\", \"LGBM\"]\n",
    "    values = [values_extra_trees[i], values_lgbm[i]]\n",
    "    \n",
    "    # Set specific y-axis limits for R¬≤\n",
    "    if metric == \"R¬≤\":\n",
    "        y_min, y_max = 0.65, 0.72\n",
    "    else:\n",
    "        \n",
    "        rounded_values = [round(val, 2) for val in values]\n",
    "        y_min, y_max = min(rounded_values) - 0.1, max(rounded_values) + 0.1\n",
    "    ax.set_ylim(y_min, y_max)\n",
    "    \n",
    "    \n",
    "    ax.spines['top'].set_visible(False)\n",
    "    ax.spines['right'].set_visible(False)\n",
    "    ax.spines['bottom'].set_visible(False)\n",
    "    ax.spines['left'].set_visible(True)\n",
    "    ax.spines['left'].set_color('black')\n",
    "    ax.spines['left'].set_linewidth(1)\n",
    "    ax.grid(False)  \n",
    "    \n",
    "    \n",
    "    y_ticks = np.linspace(y_min, y_max, num=5)\n",
    "    ax.set_yticks(y_ticks)\n",
    "    ax.set_yticklabels([f\"{tick:.2f}\" for tick in y_ticks], fontsize=10)\n",
    "    \n",
    "    \n",
    "    for j, val in enumerate(values):\n",
    "        ax.scatter(x_center, val, color=colors[j], s=100)\n",
    "        ax.text(x_center + 0.05, val, f\"{val:.3f}\", ha='left', va='center', fontsize=10, color='black')\n",
    "    \n",
    "    ax.set_xticks([])  \n",
    "    ax.set_ylabel(\"Values\")  \n",
    "    ax.set_title(f\"{metric} Comparison\", pad=20)\n",
    "    ax.set_facecolor('white')  \n",
    "\n",
    "\n",
    "legend_handles = [plt.Line2D([0], [0], marker='o', color='w', markerfacecolor=colors[0], markersize=10, label=\"ExtraTrees\"),\n",
    "                  plt.Line2D([0], [0], marker='o', color='w', markerfacecolor=colors[1], markersize=10, label=\"LGBM\")]\n",
    "fig.legend(handles=legend_handles, loc=\"upper right\", bbox_to_anchor=(1.15, 1.05), frameon=False, markerscale=1.5, fontsize=12, \n",
    "           handletextpad=1, labelspacing=1, borderpad=1)\n",
    "\n",
    "\n",
    "for i in range(len(metrics), len(axes)):\n",
    "    fig.delaxes(axes[i])\n",
    "\n",
    "plt.tight_layout()\n",
    "\n",
    "\n",
    "# Define save path\n",
    "save_path = os.path.join(\"..\", \"notebooks\", \"plots\", \"model_comparison_stats.png\")\n",
    "plt.savefig(save_path, dpi=300, bbox_inches='tight')\n",
    "print(f\"Figure saved at: {save_path}\")\n",
    "plt.show()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The comparative analysis between the ExtraTrees and LGBM models reveals distinct performance characteristics across key evaluation metrics.\n",
    "\n",
    "**Mean Error** : The ExtraTrees model shows a slightly larger absolute error (-0.034) compared to LGBM (-0.005), indicating that LGBM produces predictions that are closer to the expected values on average.\n",
    "\n",
    "**Variance**: The variance is comparable between both models, with ExtraTrees exhibiting 1.921 and LGBM 1.883, suggesting similar levels of model variability.\n",
    "\n",
    "**Mean Predicted Value**: Both models yield nearly identical predicted values, with LGBM slightly higher at 6.783 compared to 6.754 for ExtraTrees.\n",
    "\n",
    "**Mean Squared Error (MSE)**: ExtraTrees has a marginally higher MSE (1.922) compared to LGBM (1.884), suggesting that the LGBM model achieves slightly better prediction accuracy.\n",
    "\n",
    "**R¬≤ Score**: The ExtraTrees model has an R¬≤ of 0.674, while LGBM scores 0.680, reinforcing that both models explain the variance in the data quite well, but LGBM performs slightly better in this regard.\n",
    "\n",
    "**Confidence Intervals**:\n",
    "\n",
    "The 95% CI lower bound is slightly lower for ExtraTrees (-0.275) than for LGBM (-0.244).\n",
    "The 95% CI upper bound is 0.208 for ExtraTrees and 0.234 for LGBM, indicating a slightly wider confidence interval for LGBM.\n",
    "Conclusion\n",
    "While both models exhibit comparable performance, LGBM slightly outperforms ExtraTrees in most metrics, particularly in terms of lower mean error, higher R¬≤, and marginally better MSE. However, the differences are not drastic, and the choice between these models may depend on factors such as computational efficiency and interpretability."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üßÖ *Saving the LGBMRegressor trained model*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Ensure the models directory exists\n",
    "os.makedirs(\"models\", exist_ok=True)\n",
    "\n",
    "# Save the best model\n",
    "with open(\"models/best_pKa_model.pkl\", \"wb\") as file:\n",
    "    pickle.dump(model2, file)\n",
    "\n",
    "print(\"‚úÖ Best model saved as 'models/best_pKa_model.pkl'\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ü©∑ *Results: Usage of this trained machine learning model*\n",
    "As illustrated below, the functionalities provided by the modules of the *pKaPredict* package allow users to input the SMILES string of a target molecule, which is then automatically converted into molecular descriptors. Based on these descriptors, the predicted pKa value is thus computed and displayed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Exact descriptor list used during training\n",
    "descriptor_names = [ 'MaxAbsEStateIndex', 'MaxEStateIndex', 'MinAbsEStateIndex', 'MinEStateIndex', 'SPS', \n",
    "    'NumValenceElectrons', 'MaxPartialCharge', 'MinPartialCharge', 'MaxAbsPartialCharge', 'MinAbsPartialCharge', \n",
    "    'FpDensityMorgan1', 'FpDensityMorgan2', 'FpDensityMorgan3', 'BCUT2D_MWHI', 'BCUT2D_MWLOW', \n",
    "    'BCUT2D_CHGLO', 'BCUT2D_LOGPLOW', 'BCUT2D_MRHI', 'BalabanJ', 'BertzCT', 'Chi0', 'Chi0n', 'Chi1', \n",
    "    'Chi1n', 'Chi3v', 'Chi4v', 'HallKierAlpha', 'Kappa1', 'Kappa2', 'Kappa3', 'LabuteASA', 'PEOE_VSA1', \n",
    "    'PEOE_VSA12', 'PEOE_VSA13', 'PEOE_VSA14', 'PEOE_VSA3', 'PEOE_VSA4', 'PEOE_VSA5', 'PEOE_VSA6', \n",
    "    'PEOE_VSA7', 'PEOE_VSA8', 'PEOE_VSA9', 'SMR_VSA1', 'SMR_VSA10', 'SMR_VSA2', 'SMR_VSA3', 'SMR_VSA4', \n",
    "    'SMR_VSA5', 'SMR_VSA6', 'SMR_VSA7', 'SMR_VSA9', 'SlogP_VSA1', 'SlogP_VSA10', 'SlogP_VSA11', \n",
    "    'SlogP_VSA12', 'SlogP_VSA2', 'SlogP_VSA3', 'SlogP_VSA4', 'SlogP_VSA5', 'SlogP_VSA6', 'SlogP_VSA7', \n",
    "    'SlogP_VSA8', 'TPSA', 'EState_VSA1', 'EState_VSA10', 'EState_VSA2', 'EState_VSA3', 'EState_VSA4', \n",
    "    'EState_VSA5', 'EState_VSA6', 'EState_VSA7', 'EState_VSA8', 'EState_VSA9', 'VSA_EState1', \n",
    "    'VSA_EState10', 'VSA_EState2', 'VSA_EState3', 'VSA_EState4', 'VSA_EState6', 'VSA_EState7', \n",
    "    'VSA_EState8', 'VSA_EState9', 'FractionCSP3', 'NHOHCount', 'NOCount', 'NumAliphaticHeterocycles', \n",
    "    'NumAliphaticRings', 'NumAromaticHeterocycles', 'NumAromaticRings', 'NumBridgeheadAtoms', \n",
    "    'NumHAcceptors', 'NumHeteroatoms', 'NumHeterocycles', 'NumRotatableBonds', 'NumSaturatedHeterocycles', \n",
    "    'NumSaturatedRings', 'Phi', 'MolMR', 'fr_Al_COO', 'fr_ArN', 'fr_Ar_COO', 'fr_Ar_N', 'fr_Ar_OH', \n",
    "    'fr_COO', 'fr_COO2', 'fr_C_O', 'fr_C_O_noCOO', 'fr_C_S', 'fr_HOCCN', 'fr_Imine', 'fr_NH0', \n",
    "    'fr_NH1', 'fr_NH2', 'fr_N_O', 'fr_Ndealkylation1', 'fr_Ndealkylation2', 'fr_alkyl_halide', \n",
    "    'fr_allylic_oxid', 'fr_amidine', 'fr_aniline', 'fr_aryl_methyl', 'fr_azide', 'fr_azo', 'fr_ester', \n",
    "    'fr_ether', 'fr_guanido', 'fr_halogen', 'fr_imidazole', 'fr_lactam', 'fr_methoxy', 'fr_nitrile', \n",
    "    'fr_nitroso', 'fr_phenol', 'fr_phenol_noOrthoHbond', 'fr_piperdine', 'fr_pyridine', 'fr_quatN', \n",
    "    'fr_sulfide', 'fr_sulfonamd', 'fr_sulfone', 'fr_tetrazole', 'fr_thiazole'\n",
    "]\n",
    "\n",
    "from pkapredict.predict_pKa import predict_pKa \n",
    "from pkapredict.load_model import load_model\n",
    "def main():\n",
    "    print(\"üî¨ Welcome to the pKa Predictor!\")\n",
    "    smiles = input(\"üëâ Enter a SMILES string (e.g., [NH4+]): \").strip()\n",
    "\n",
    "    try:\n",
    "        model = load_model()\n",
    "        predicted_pKa = predict_pKa(smiles, model, descriptor_names)\n",
    "        print(f\"\\n‚úÖ Predicted pKa for {smiles}: {predicted_pKa:.2f}\")\n",
    "    except ValueError as ve:\n",
    "        print(str(ve))\n",
    "    except FileNotFoundError:\n",
    "        print(\"‚ùå Model file not found. Please check the path.\")\n",
    "    except Exception as e:\n",
    "        print(f\"‚ö†Ô∏è Unexpected error: {e}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üí¨ *Discussion*\n",
    "\n",
    "The *pKaPredict* package demonstrates promising predictive performance, with the LGBMRegressor model achieving an R¬≤ value of 0.68 and a root mean squared error (RMSE) of 1.37 on the validation set. These metrics suggest that while the model captures a substantial portion of the variance in the data, notable deviations from experimental values remain. An R¬≤ value of 0.68 indicates that approximately 68% of the variance in the experimental pKa measurements is accounted for by the model, reflecting a moderate degree of correlation. However, this also implies that around 32% of the variance remains unexplained, suggesting room for improvement. Additionally, the root mean squared error (RMSE) of 1.37 indicates that, on average, the predicted pKa values deviate from the true values by more than one unit which is a non-negligible margin. The dispersion of data points around the regression line further highlights that certain molecules, particularly those with higher pKa values, are subject to more pronounced over- or under-predictions, pointing to potential challenges in generalizing across the full range of chemical structures present in the dataset.\n",
    "\n",
    "Another factor that can account for the limitations of the package in predictive accuracy is training data imbalance. The distribution of pKa values in the dataset is not uniform. Certain pKa ranges, particularly around values of 4‚Äì5 and 9‚Äì10‚Äîare more densely represented than others. This imbalance can lead the model to be biased toward frequently occurring ranges, reducing its accuracy for less represented values.\n",
    "\n",
    "Moreover, although the LGBMRegressor is a powerful gradient boosting algorithm, it is still a data-driven model. Its performance is heavily dependent on the diversity and quality of the training set. In cases where the chemical structures or functional groups of the test molecules differ significantly from those in the training set, the model may struggle to generalize and produce reliable predictions.\n",
    "\n",
    "In summary, while the *pKaPredict* package provides a functional and efficient pipeline for estimating pKa values, the accuracy of its predictions is constrained by the training data imbalance and the data-driven nature of the model. Future improvements may include enriching the training dataset with more diverse molecular structures, and exploring ensemble modeling approaches to enhance predictive robustness."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pKaPredict",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
